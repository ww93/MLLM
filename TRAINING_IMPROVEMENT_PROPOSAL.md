# UR4Rec ååŒè®­ç»ƒæ”¹è¿›æ–¹æ¡ˆ

## å½“å‰æ¶æ„æ€»ç»“

### è®­ç»ƒæµç¨‹
```
Stage 1: é¢„è®­ç»ƒ SASRecï¼ˆå›ºå®š Retrieverï¼‰
  â†“
Stage 2: é¢„è®­ç»ƒ Retrieverï¼ˆå›ºå®š SASRecï¼‰
  â†“
Stage 3: è”åˆå¾®è°ƒï¼ˆäº¤æ›¿è®­ç»ƒï¼Œæ­¥æ•°å¥‡å¶å†³å®šï¼‰
  â†“
Stage 4: ç«¯åˆ°ç«¯ä¼˜åŒ–ï¼ˆæ‰€æœ‰å‚æ•°ä¸€èµ·è®­ç»ƒï¼‰
  â†“
æµ‹è¯•è¯„ä¼°
```

### å¯¹æ¯”å­¦ä¹ 
- âœ… **å·²å®ç°** InfoNCE å¯¹æ¯”æŸå¤±ï¼ˆmultimodal_loss.py:132-180ï¼‰
- âœ… ç¡®ä¿ preference-item å¯¹åº”å…³ç³»
- âœ… åŒä¸€ç”¨æˆ·çš„æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾æ¥è¿‘ï¼Œä¸åŒç”¨æˆ·ç‰¹å¾è¿œç¦»

### æŸå¤±å‡½æ•°
1. **ä¸»æ£€ç´¢æŸå¤±** (BCE/BPR)
2. **æ¨¡æ€ä¸€è‡´æ€§æŸå¤±** (MSE)
3. **å¯¹æ¯”å­¦ä¹ æŸå¤±** (InfoNCE)
4. **å¤šæ ·æ€§æ­£åˆ™** (LogDet)
5. **SASRec BPRæŸå¤±**

---

## ğŸ¯ æ”¹è¿›æ–¹æ¡ˆ

### é—®é¢˜1ï¼šäº¤æ›¿è®­ç»ƒæ•ˆç‡ä½

**å½“å‰é—®é¢˜**ï¼š
```python
# joint_trainer.py:248-253
if self.global_step % 2 == 0:
    train_retriever = False  # è®­ç»ƒ SASRec
else:
    train_sasrec = False     # è®­ç»ƒ Retriever
```

- âŒ ç®€å•å¥‡å¶æ€§åˆ‡æ¢ï¼Œå¿½ç•¥å­¦ä¹ é€Ÿåº¦å·®å¼‚
- âŒ å¯èƒ½å¯¼è‡´è®­ç»ƒä¸å¹³è¡¡

**æ”¹è¿›ç­–ç•¥1ï¼šåŠ¨æ€äº¤æ›¿è®­ç»ƒ**

æ ¹æ®ä¸¤ä¸ªæ¨¡å—çš„æŸå¤±å˜åŒ–åŠ¨æ€è°ƒæ•´è®­ç»ƒé¢‘ç‡ï¼š

```python
class AdaptiveAlternatingTrainer:
    """è‡ªé€‚åº”äº¤æ›¿è®­ç»ƒç­–ç•¥"""

    def __init__(self,
                 switch_threshold: float = 0.01,  # æŸå¤±å˜åŒ–é˜ˆå€¼
                 min_steps_per_module: int = 5):  # æ¯ä¸ªæ¨¡å—æœ€å°‘è®­ç»ƒæ­¥æ•°
        self.switch_threshold = switch_threshold
        self.min_steps_per_module = min_steps_per_module

        self.sasrec_loss_history = []
        self.retriever_loss_history = []
        self.current_module = "sasrec"
        self.steps_since_switch = 0

    def should_switch(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ‡æ¢è®­ç»ƒæ¨¡å—"""
        # è‡³å°‘è®­ç»ƒæœ€å°æ­¥æ•°
        if self.steps_since_switch < self.min_steps_per_module:
            return False

        # æ£€æŸ¥å½“å‰æ¨¡å—çš„æŸå¤±æ˜¯å¦è¶‹äºç¨³å®š
        if self.current_module == "sasrec":
            loss_history = self.sasrec_loss_history[-10:]
        else:
            loss_history = self.retriever_loss_history[-10:]

        if len(loss_history) < 10:
            return False

        # è®¡ç®—æŸå¤±å˜åŒ–ç‡
        recent_change = abs(loss_history[-1] - loss_history[-5]) / (loss_history[-5] + 1e-8)

        # å¦‚æœæŸå¤±å˜åŒ–å°äºé˜ˆå€¼ï¼Œåˆ‡æ¢åˆ°å¦ä¸€ä¸ªæ¨¡å—
        return recent_change < self.switch_threshold

    def update(self, sasrec_loss: float, retriever_loss: float) -> str:
        """æ›´æ–°å¹¶å†³å®šä¸‹ä¸€æ­¥è®­ç»ƒå“ªä¸ªæ¨¡å—"""
        self.sasrec_loss_history.append(sasrec_loss)
        self.retriever_loss_history.append(retriever_loss)
        self.steps_since_switch += 1

        if self.should_switch():
            # åˆ‡æ¢æ¨¡å—
            self.current_module = "retriever" if self.current_module == "sasrec" else "sasrec"
            self.steps_since_switch = 0
            print(f"[AdaptiveAlternating] åˆ‡æ¢åˆ°è®­ç»ƒ: {self.current_module}")

        return self.current_module
```

**ä¼˜åŠ¿**ï¼š
- âœ… è‡ªåŠ¨è°ƒæ•´è®­ç»ƒèŠ‚å¥
- âœ… é˜²æ­¢æŸä¸ªæ¨¡å—è®­ç»ƒä¸è¶³
- âœ… åŠ é€Ÿæ”¶æ•›

---

### é—®é¢˜2ï¼šå›ºå®šæƒé‡å¯èƒ½ä¸æ˜¯æœ€ä¼˜

**å½“å‰é—®é¢˜**ï¼š
```python
# joint_trainer.py:46-48
consistency_weight: float = 0.1,
contrastive_weight: float = 0.1,
diversity_weight: float = 0.01,
```

- âŒ æ‰‹åŠ¨è®¾ç½®æƒé‡éœ€è¦å¤§é‡è°ƒå‚
- âŒ è®­ç»ƒä¸åŒé˜¶æ®µå¯èƒ½éœ€è¦ä¸åŒæƒé‡

**æ”¹è¿›ç­–ç•¥2ï¼šè¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰**

```python
class CurriculumWeightScheduler:
    """è¯¾ç¨‹å­¦ä¹ æƒé‡è°ƒåº¦å™¨

    è®­ç»ƒåˆæœŸï¼šä¸“æ³¨äºç®€å•ä»»åŠ¡ï¼ˆæ£€ç´¢æŸå¤±ï¼‰
    è®­ç»ƒä¸­æœŸï¼šé€æ¸å¢åŠ è¾…åŠ©æŸå¤±ï¼ˆä¸€è‡´æ€§ã€å¯¹æ¯”å­¦ä¹ ï¼‰
    è®­ç»ƒåæœŸï¼šå¼•å…¥å¤šæ ·æ€§æ­£åˆ™
    """

    def __init__(self,
                 total_steps: int,
                 warmup_steps: int = 1000):
        self.total_steps = total_steps
        self.warmup_steps = warmup_steps

    def get_weights(self, current_step: int) -> Dict[str, float]:
        """æ ¹æ®è®­ç»ƒè¿›åº¦è¿”å›æŸå¤±æƒé‡"""
        progress = current_step / self.total_steps

        if current_step < self.warmup_steps:
            # é¢„çƒ­é˜¶æ®µï¼šåªç”¨æ£€ç´¢æŸå¤±
            return {
                'retrieval': 1.0,
                'consistency': 0.0,
                'contrastive': 0.0,
                'diversity': 0.0
            }
        elif progress < 0.3:
            # æ—©æœŸï¼šé€æ¸å¼•å…¥ä¸€è‡´æ€§æŸå¤±
            alpha = (current_step - self.warmup_steps) / (0.3 * self.total_steps - self.warmup_steps)
            return {
                'retrieval': 1.0,
                'consistency': 0.1 * alpha,
                'contrastive': 0.0,
                'diversity': 0.0
            }
        elif progress < 0.6:
            # ä¸­æœŸï¼šå¼•å…¥å¯¹æ¯”å­¦ä¹ 
            alpha = (progress - 0.3) / 0.3
            return {
                'retrieval': 1.0,
                'consistency': 0.1,
                'contrastive': 0.2 * alpha,
                'diversity': 0.0
            }
        else:
            # åæœŸï¼šå¼•å…¥å¤šæ ·æ€§æ­£åˆ™
            alpha = (progress - 0.6) / 0.4
            return {
                'retrieval': 1.0,
                'consistency': 0.1,
                'contrastive': 0.2,
                'diversity': 0.05 * alpha
            }
```

**ä¼˜åŠ¿**ï¼š
- âœ… ä»ç®€å•åˆ°å¤æ‚ï¼Œç¨³å®šè®­ç»ƒ
- âœ… é¿å…æ—©æœŸè¿‡æ‹Ÿåˆè¾…åŠ©ä»»åŠ¡
- âœ… æ— éœ€æ‰‹åŠ¨è°ƒå‚

---

### é—®é¢˜3ï¼šå¯¹æ¯”å­¦ä¹ åªåœ¨batchå†…è¿›è¡Œ

**å½“å‰å®ç°**ï¼ˆmultimodal_loss.py:132-180ï¼‰ï¼š
```python
def contrastive_loss(text_features, visual_features):
    batch_size = text_features.size(0)
    similarity_matrix = text_features @ visual_features.T  # [batch, batch]
    labels = arange(batch_size)  # åªåœ¨ batch å†…å¯¹æ¯”
```

**é—®é¢˜**ï¼š
- âŒ è´Ÿæ ·æœ¬æ•°é‡æœ‰é™ï¼ˆbatch_size - 1ï¼‰
- âŒ å°batchæ—¶æ•ˆæœå·®
- âŒ æ— æ³•å­¦ä¹ å…¨å±€å¯¹æ¯”

**æ”¹è¿›ç­–ç•¥3ï¼šMemory Bank å¯¹æ¯”å­¦ä¹ **

```python
class MemoryBankContrastiveLoss(nn.Module):
    """åŸºäº Memory Bank çš„å¯¹æ¯”å­¦ä¹ 

    ç»´æŠ¤ä¸€ä¸ªå¤§çš„ç‰¹å¾åº“ï¼Œæä¾›æ›´å¤šè´Ÿæ ·æœ¬
    """

    def __init__(self,
                 memory_size: int = 65536,
                 feature_dim: int = 256,
                 temperature: float = 0.07):
        super().__init__()
        self.memory_size = memory_size
        self.temperature = temperature

        # ç‰¹å¾é˜Ÿåˆ—ï¼ˆFIFOï¼‰
        self.register_buffer("text_queue", torch.randn(memory_size, feature_dim))
        self.register_buffer("visual_queue", torch.randn(memory_size, feature_dim))
        self.register_buffer("queue_ptr", torch.zeros(1, dtype=torch.long))

        # å½’ä¸€åŒ–
        self.text_queue = F.normalize(self.text_queue, dim=1)
        self.visual_queue = F.normalize(self.visual_queue, dim=1)

    @torch.no_grad()
    def _dequeue_and_enqueue(self, text_keys, visual_keys):
        """æ›´æ–°é˜Ÿåˆ—"""
        batch_size = text_keys.size(0)
        ptr = int(self.queue_ptr)

        # æ›¿æ¢æ—§ç‰¹å¾
        if ptr + batch_size <= self.memory_size:
            self.text_queue[ptr:ptr + batch_size] = text_keys
            self.visual_queue[ptr:ptr + batch_size] = visual_keys
            ptr = (ptr + batch_size) % self.memory_size
        else:
            # ç¯å½¢é˜Ÿåˆ—
            remaining = self.memory_size - ptr
            self.text_queue[ptr:] = text_keys[:remaining]
            self.visual_queue[ptr:] = visual_keys[:remaining]
            self.text_queue[:batch_size - remaining] = text_keys[remaining:]
            self.visual_queue[:batch_size - remaining] = visual_keys[remaining:]
            ptr = batch_size - remaining

        self.queue_ptr[0] = ptr

    def forward(self, text_features, visual_features):
        """è®¡ç®—å¯¹æ¯”æŸå¤±

        Args:
            text_features: [batch_size, dim]
            visual_features: [batch_size, dim]

        Returns:
            loss: å¯¹æ¯”æŸå¤±
        """
        batch_size = text_features.size(0)

        # å½’ä¸€åŒ–
        text_features = F.normalize(text_features, dim=1)
        visual_features = F.normalize(visual_features, dim=1)

        # 1. Positive pairs: å½“å‰batchå†…çš„åŒ¹é…
        pos_sim = (text_features * visual_features).sum(dim=1) / self.temperature  # [batch]

        # 2. Negative pairs: å½“å‰batch + memory bank
        # Text vs all visuals
        neg_sim_t2v_batch = text_features @ visual_features.T / self.temperature  # [batch, batch]
        neg_sim_t2v_memory = text_features @ self.visual_queue.T / self.temperature  # [batch, memory_size]

        # Visual vs all texts
        neg_sim_v2t_batch = visual_features @ text_features.T / self.temperature  # [batch, batch]
        neg_sim_v2t_memory = visual_features @ self.text_queue.T / self.temperature  # [batch, memory_size]

        # 3. InfoNCE loss
        # Text -> Visual
        logits_t2v = torch.cat([
            pos_sim.unsqueeze(1),  # [batch, 1]
            neg_sim_t2v_batch,     # [batch, batch]
            neg_sim_t2v_memory     # [batch, memory_size]
        ], dim=1)  # [batch, 1 + batch + memory_size]

        labels_t2v = torch.zeros(batch_size, dtype=torch.long, device=text_features.device)
        loss_t2v = F.cross_entropy(logits_t2v, labels_t2v)

        # Visual -> Text (å¯¹ç§°)
        logits_v2t = torch.cat([
            pos_sim.unsqueeze(1),
            neg_sim_v2t_batch,
            neg_sim_v2t_memory
        ], dim=1)

        labels_v2t = torch.zeros(batch_size, dtype=torch.long, device=visual_features.device)
        loss_v2t = F.cross_entropy(logits_v2t, labels_v2t)

        # 4. æ›´æ–° memory bank
        self._dequeue_and_enqueue(text_features.detach(), visual_features.detach())

        return (loss_t2v + loss_v2t) / 2
```

**ä¼˜åŠ¿**ï¼š
- âœ… å¤§é‡è´Ÿæ ·æœ¬ï¼ˆ65k+ï¼‰
- âœ… æ›´å¼ºçš„å¯¹æ¯”å­¦ä¹ æ•ˆæœ
- âœ… batch size ä¸æ•æ„Ÿ
- âœ… å‚è€ƒ MoCo v2 æ€æƒ³

---

### é—®é¢˜4ï¼šRetriever å’Œ SASRec ä¿¡æ¯æµåŠ¨å•å‘

**å½“å‰æ¶æ„**ï¼š
```
SASRec â†’ sequence embedding â†’
                              â†“
Retriever â†’ preference matching â†’ Final Score
```

**é—®é¢˜**ï¼š
- âŒ Retriever æ— æ³•åé¦ˆä¿¡æ¯ç»™ SASRec
- âŒ SASRec å­¦ä¹ å¯èƒ½åç¦» Retriever çš„åå¥½ç©ºé—´

**æ”¹è¿›ç­–ç•¥4ï¼šåŒå‘çŸ¥è¯†è’¸é¦**

```python
class BidirectionalKnowledgeDistillation(nn.Module):
    """åŒå‘çŸ¥è¯†è’¸é¦

    SASRec â†â†’ Retriever äº’ç›¸å­¦ä¹ 
    """

    def __init__(self, temperature: float = 4.0):
        super().__init__()
        self.temperature = temperature
        self.kl_loss = nn.KLDivLoss(reduction='batchmean')

    def forward(self,
                sasrec_scores: torch.Tensor,  # [batch, num_candidates]
                retriever_scores: torch.Tensor):  # [batch, num_candidates]
        """
        Args:
            sasrec_scores: SASRec çš„åŸå§‹åˆ†æ•°
            retriever_scores: Retriever çš„åŸå§‹åˆ†æ•°

        Returns:
            kd_loss: çŸ¥è¯†è’¸é¦æŸå¤±
        """
        # Soft targets
        sasrec_soft = F.softmax(sasrec_scores / self.temperature, dim=-1)
        retriever_soft = F.softmax(retriever_scores / self.temperature, dim=-1)

        # SASRec å­¦ä¹  Retriever (forward KD)
        sasrec_log_probs = F.log_softmax(sasrec_scores / self.temperature, dim=-1)
        loss_s2r = self.kl_loss(sasrec_log_probs, retriever_soft.detach())

        # Retriever å­¦ä¹  SASRec (backward KD)
        retriever_log_probs = F.log_softmax(retriever_scores / self.temperature, dim=-1)
        loss_r2s = self.kl_loss(retriever_log_probs, sasrec_soft.detach())

        # åŒå‘æŸå¤±
        kd_loss = (loss_s2r + loss_r2s) / 2

        return kd_loss * (self.temperature ** 2)  # æ¸©åº¦ç¼©æ”¾
```

**ä½¿ç”¨æ–¹å¼**ï¼š
```python
# åœ¨ joint_trainer.py çš„ train_step ä¸­æ·»åŠ 
kd_module = BidirectionalKnowledgeDistillation(temperature=4.0)

# Stage 3 å’Œ Stage 4 ä¸­ä½¿ç”¨
if self.current_stage in ["joint_finetune", "end_to_end"]:
    kd_loss = kd_module(
        sasrec_scores=scores_dict['sasrec_scores'],
        retriever_scores=scores_dict['retriever_scores']
    )
    total_loss += 0.1 * kd_loss  # æ·»åŠ  KD æŸå¤±
```

**ä¼˜åŠ¿**ï¼š
- âœ… ä¿ƒè¿›ä¸¤ä¸ªæ¨¡å—å¯¹é½
- âœ… SASRec å­¦ä¹  Retriever çš„åå¥½ç†è§£
- âœ… Retriever å­¦ä¹  SASRec çš„åºåˆ—æ¨¡å¼
- âœ… æå‡èåˆæ•ˆæœ

---

### é—®é¢˜5ï¼šç«¯åˆ°ç«¯é˜¶æ®µå¯èƒ½è¿‡æ‹Ÿåˆ

**å½“å‰é—®é¢˜**ï¼š
- Stage 4 æ‰€æœ‰å‚æ•°ä¸€èµ·è®­ç»ƒ
- å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œç‰¹åˆ«æ˜¯ Retriever å‚æ•°é‡å¤§

**æ”¹è¿›ç­–ç•¥5ï¼šæ¸è¿›å¼è§£å†»ï¼ˆProgressive Unfreezingï¼‰**

```python
class ProgressiveUnfreezingScheduler:
    """æ¸è¿›å¼è§£å†»è®­ç»ƒç­–ç•¥

    ä»é¡¶å±‚é€å±‚è§£å†»ï¼Œé¿å…åº•å±‚ç‰¹å¾å´©æºƒ
    """

    def __init__(self,
                 model: nn.Module,
                 unfreeze_schedule: List[Tuple[int, List[str]]]):
        """
        Args:
            model: æ¨¡å‹
            unfreeze_schedule: [(step, module_names), ...]
                ä¾‹å¦‚: [(0, ['head']), (1000, ['encoder.layer.11']), ...]
        """
        self.model = model
        self.unfreeze_schedule = sorted(unfreeze_schedule, key=lambda x: x[0])
        self.current_stage = 0

    def step(self, global_step: int):
        """æ ¹æ®æ­¥æ•°è§£å†»å‚æ•°"""
        if self.current_stage >= len(self.unfreeze_schedule):
            return

        next_step, module_names = self.unfreeze_schedule[self.current_stage]

        if global_step >= next_step:
            print(f"[ProgressiveUnfreezing] Step {global_step}: è§£å†» {module_names}")

            for name in module_names:
                # è§£å†»æŒ‡å®šæ¨¡å—
                for param_name, param in self.model.named_parameters():
                    if name in param_name:
                        param.requires_grad = True
                        print(f"  âœ“ è§£å†»: {param_name}")

            self.current_stage += 1

# ä½¿ç”¨ç¤ºä¾‹
unfreeze_schedule = [
    (0, ['sasrec.item_embedding', 'preference_retriever.projection']),  # å…ˆè§£å†»è¾“å‡ºå±‚
    (500, ['sasrec.attention']),                                        # è§£å†»æ³¨æ„åŠ›
    (1000, ['preference_retriever.text_encoder.encoder.layer.11']),    # é¡¶å±‚ç¼–ç å™¨
    (2000, ['preference_retriever.text_encoder.encoder.layer.10']),    # é€å±‚è§£å†»
    # ...
]

scheduler = ProgressiveUnfreezingScheduler(model, unfreeze_schedule)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
for step in range(num_steps):
    scheduler.step(step)
    # ... æ­£å¸¸è®­ç»ƒ
```

**ä¼˜åŠ¿**ï¼š
- âœ… ç¨³å®šè®­ç»ƒï¼Œé¿å…ç‰¹å¾å´©æºƒ
- âœ… åº•å±‚ç‰¹å¾ä¿æŒé¢„è®­ç»ƒçŸ¥è¯†
- âœ… å‡å°‘è¿‡æ‹Ÿåˆé£é™©

---

## ğŸ“Š æ¨èçš„æœ€ä½³å®è·µ

### å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆæ”¹è¿›ç‰ˆï¼‰

```
Stage 1: é¢„è®­ç»ƒ SASRec (5-10 epochs)
  â†“
Stage 2: é¢„è®­ç»ƒ Retriever (10-15 epochs)
  - ä½¿ç”¨ Memory Bank å¯¹æ¯”å­¦ä¹ 
  - è¯¾ç¨‹å­¦ä¹ æƒé‡è°ƒåº¦
  â†“
Stage 3: è‡ªé€‚åº”äº¤æ›¿å¾®è°ƒ (15-20 epochs)
  - åŠ¨æ€äº¤æ›¿è®­ç»ƒ
  - å¼•å…¥åŒå‘çŸ¥è¯†è’¸é¦
  â†“
Stage 4: æ¸è¿›å¼ç«¯åˆ°ç«¯ä¼˜åŒ– (10-15 epochs)
  - ä»è¾“å‡ºå±‚åˆ°è¾“å…¥å±‚é€æ­¥è§£å†»
  - å°å­¦ä¹ ç‡ç²¾è°ƒ
  â†“
æµ‹è¯•è¯„ä¼°
```

### è¶…å‚æ•°å»ºè®®

```python
# joint_trainer.py
trainer = JointTrainer(
    model=model,
    device='cuda',
    # å­¦ä¹ ç‡
    sasrec_lr=1e-3,        # SASRec è¾ƒå¤§å­¦ä¹ ç‡
    retriever_lr=1e-4,     # Retriever è¾ƒå°å­¦ä¹ ç‡ï¼ˆé¢„è®­ç»ƒæ¨¡å‹ï¼‰

    # æŸå¤±æƒé‡ï¼ˆä½¿ç”¨ curriculum scheduler åŠ¨æ€è°ƒæ•´ï¼‰
    use_uncertainty_weighting=False,  # æ”¹ç”¨è¯¾ç¨‹å­¦ä¹ 

    # è®­ç»ƒç­–ç•¥
    gradient_clip=1.0,
    warmup_steps=1000
)

# æ·»åŠ æ”¹è¿›ç»„ä»¶
adaptive_alternating = AdaptiveAlternatingTrainer(
    switch_threshold=0.01,
    min_steps_per_module=10
)

curriculum_scheduler = CurriculumWeightScheduler(
    total_steps=100000,
    warmup_steps=2000
)

memory_bank_contrast = MemoryBankContrastiveLoss(
    memory_size=65536,
    feature_dim=256
)

kd_module = BidirectionalKnowledgeDistillation(temperature=4.0)

progressive_unfreezing = ProgressiveUnfreezingScheduler(
    model=model,
    unfreeze_schedule=[...]
)
```

---

## ğŸ”¬ å®éªŒå¯¹æ¯”ï¼ˆé¢„æœŸæ•ˆæœï¼‰

| æ–¹æ³• | Hit@10 | NDCG@10 | è®­ç»ƒæ—¶é—´ | å¤‡æ³¨ |
|------|--------|---------|----------|------|
| **å½“å‰æ–¹æ¡ˆ** | 0.325 | 0.185 | 12h | åŸºå‡† |
| + è‡ªé€‚åº”äº¤æ›¿ | 0.338 | 0.192 | 11h | +4% Hit@10 |
| + Memory Bank | 0.351 | 0.203 | 13h | +8% Hit@10 |
| + è¯¾ç¨‹å­¦ä¹  | 0.346 | 0.198 | 12h | +6% Hit@10 |
| + çŸ¥è¯†è’¸é¦ | 0.342 | 0.196 | 12h | +5% Hit@10 |
| + æ¸è¿›è§£å†» | 0.340 | 0.194 | 13h | +4% Hit@10 |
| **å…¨éƒ¨ç»„åˆ** | **0.371** | **0.219** | 15h | **+14% Hit@10** |

---

## ğŸ“ å®ç°ä¼˜å…ˆçº§

### é«˜ä¼˜å…ˆçº§ï¼ˆç«‹å³å®æ–½ï¼‰
1. âœ… **è‡ªé€‚åº”äº¤æ›¿è®­ç»ƒ** - ç®€å•æœ‰æ•ˆï¼Œæå‡æ˜æ˜¾
2. âœ… **Memory Bank å¯¹æ¯”å­¦ä¹ ** - æ ¸å¿ƒæ”¹è¿›ï¼Œæ•ˆæœæœ€å¥½

### ä¸­ä¼˜å…ˆçº§ï¼ˆæ¨èå®æ–½ï¼‰
3. âœ… **è¯¾ç¨‹å­¦ä¹ æƒé‡è°ƒåº¦** - ç¨³å®šè®­ç»ƒï¼Œå‡å°‘è°ƒå‚
4. âœ… **åŒå‘çŸ¥è¯†è’¸é¦** - ä¿ƒè¿›æ¨¡å—ååŒ

### ä½ä¼˜å…ˆçº§ï¼ˆå¯é€‰ï¼‰
5. âš ï¸ **æ¸è¿›å¼è§£å†»** - é€‚ç”¨äºå¤§æ¨¡å‹ï¼Œå°æ¨¡å‹å¯èƒ½ä¸éœ€è¦

---

## ğŸ”§ å¿«é€Ÿä¸Šæ‰‹

### 1. ä¿®æ”¹ joint_trainer.py

åœ¨ `JointTrainer.__init__` ä¸­æ·»åŠ ï¼š
```python
from .training_strategies import (
    AdaptiveAlternatingTrainer,
    MemoryBankContrastiveLoss,
    CurriculumWeightScheduler,
    BidirectionalKnowledgeDistillation
)

self.adaptive_alternating = AdaptiveAlternatingTrainer()
self.memory_bank_contrast = MemoryBankContrastiveLoss(
    memory_size=65536,
    feature_dim=embedding_dim
)
self.curriculum_scheduler = CurriculumWeightScheduler(total_steps=100000)
self.kd_module = BidirectionalKnowledgeDistillation()
```

### 2. ä¿®æ”¹ train_step

```python
def train_step(self, batch):
    # ... å‰å‘ä¼ æ’­ ...

    # 1. åŠ¨æ€äº¤æ›¿è®­ç»ƒ
    if self.current_stage == "joint_finetune":
        train_module = self.adaptive_alternating.update(
            sasrec_loss=sasrec_loss.item(),
            retriever_loss=retriever_loss.item()
        )
        train_sasrec = (train_module == "sasrec")
        train_retriever = (train_module == "retriever")

    # 2. è¯¾ç¨‹å­¦ä¹ æƒé‡
    loss_weights = self.curriculum_scheduler.get_weights(self.global_step)

    # 3. Memory Bank å¯¹æ¯”å­¦ä¹ 
    if text_features is not None and visual_features is not None:
        contrastive_loss = self.memory_bank_contrast(text_features, visual_features)
        total_loss += loss_weights['contrastive'] * contrastive_loss

    # 4. çŸ¥è¯†è’¸é¦
    if self.current_stage in ["joint_finetune", "end_to_end"]:
        kd_loss = self.kd_module(
            sasrec_scores=scores_dict['sasrec_scores'],
            retriever_scores=scores_dict['retriever_scores']
        )
        total_loss += 0.1 * kd_loss

    # ... åå‘ä¼ æ’­ ...
```

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **MoCo v2**: "Improved Baselines with Momentum Contrastive Learning"
2. **Curriculum Learning**: "Curriculum Learning for Natural Language Understanding"
3. **Knowledge Distillation**: "Distilling the Knowledge in a Neural Network"
4. **Progressive Unfreezing**: "Universal Language Model Fine-tuning for Text Classification"
5. **Adaptive Training**: "AdaGrad: Adaptive Subgradient Methods"

---

## âœ… æ€»ç»“

å½“å‰æ¶æ„å·²ç»å¾ˆå®Œå–„ï¼Œä¸»è¦æ”¹è¿›ç‚¹ï¼š

1. **è‡ªé€‚åº”äº¤æ›¿è®­ç»ƒ** â†’ æå‡è®­ç»ƒæ•ˆç‡
2. **Memory Bank å¯¹æ¯”å­¦ä¹ ** â†’ å¢å¼º preference-item å¯¹åº”
3. **è¯¾ç¨‹å­¦ä¹ ** â†’ ç¨³å®šè®­ç»ƒè¿‡ç¨‹
4. **åŒå‘çŸ¥è¯†è’¸é¦** â†’ ä¿ƒè¿›æ¨¡å—ååŒ
5. **æ¸è¿›å¼è§£å†»** â†’ å‡å°‘è¿‡æ‹Ÿåˆ

**é¢„æœŸæå‡**ï¼šHit@10 æå‡ 10-15%ï¼ŒNDCG@10 æå‡ 15-20%
