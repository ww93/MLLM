# LLM APIä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

UR4Recé¡¹ç›®æ”¯æŒå¤šç§LLMåç«¯ï¼ŒåŒ…æ‹¬ï¼š
- âœ… **OpenAIå®˜æ–¹API** (GPT-3.5, GPT-4ç­‰)
- âœ… **OpenAIå…¼å®¹API** (vLLM, LocalAI, LM Studio, Ollamaç­‰)
- âœ… **Anthropic API** (Claudeç³»åˆ—)
- âœ… **æœ¬åœ°æ¨¡å‹** (é€šè¿‡Transformersåº“)

---

## 1. OpenAIå®˜æ–¹API

### è®¾ç½®APIå¯†é’¥

```bash
export OPENAI_API_KEY="your-api-key-here"
```

### Pythonä»£ç ç¤ºä¾‹

```python
from models import OpenAILLM

# ä½¿ç”¨å®˜æ–¹OpenAI API
llm = OpenAILLM(
    model="gpt-3.5-turbo",
    api_key="your-api-key"  # æˆ–è€…ä½¿ç”¨ç¯å¢ƒå˜é‡
)

response = llm.generate("Summarize user preferences based on their history.")
print(response)
```

---

## 2. OpenAIå…¼å®¹API ğŸ†•

### æ”¯æŒçš„æœåŠ¡

| æœåŠ¡ | é»˜è®¤ç«¯å£ | base_urlç¤ºä¾‹ |
|------|---------|-------------|
| **vLLM** | 8000 | `http://localhost:8000/v1` |
| **LocalAI** | 8080 | `http://localhost:8080/v1` |
| **LM Studio** | 1234 | `http://localhost:1234/v1` |
| **Ollama** | 11434 | `http://localhost:11434/v1` |
| **Text Generation WebUI** | 5000 | `http://localhost:5000/v1` |

### ç¤ºä¾‹1: ä½¿ç”¨vLLM

```python
from models import OpenAILLM

# è¿æ¥åˆ°æœ¬åœ°vLLMæœåŠ¡
llm = OpenAILLM(
    model="meta-llama/Llama-2-7b-chat-hf",  # vLLMåŠ è½½çš„æ¨¡å‹
    api_key="dummy-key",  # æœ¬åœ°æœåŠ¡é€šå¸¸ä¸éœ€è¦çœŸå®key
    base_url="http://localhost:8000/v1"
)

response = llm.generate("Generate user preference description.")
print(response)
```

#### vLLMæœåŠ¡å™¨å¯åŠ¨

```bash
# å®‰è£…vLLM
pip install vllm

# å¯åŠ¨OpenAIå…¼å®¹æœåŠ¡å™¨
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --port 8000
```

### ç¤ºä¾‹2: ä½¿ç”¨LM Studio

```python
from models import OpenAILLM

# LM Studioæä¾›OpenAIå…¼å®¹API
llm = OpenAILLM(
    model="local-model",  # LM Studioä¸­åŠ è½½çš„æ¨¡å‹
    base_url="http://localhost:1234/v1"
)

response = llm.generate("Describe this item.")
```

#### LM Studioè®¾ç½®
1. ä¸‹è½½å¹¶å®‰è£…LM Studio
2. åŠ è½½æœ¬åœ°æ¨¡å‹
3. åœ¨è®¾ç½®ä¸­å¯ç”¨"Local Server"
4. é»˜è®¤åœ°å€: `http://localhost:1234/v1`

### ç¤ºä¾‹3: ä½¿ç”¨Ollama

```python
from models import OpenAILLM

# Ollamaéœ€è¦å¯ç”¨OpenAIå…¼å®¹æ¨¡å¼
llm = OpenAILLM(
    model="llama3.2",
    base_url="http://localhost:11434/v1"
)

response = llm.generate("Analyze user behavior.")
```

#### Ollama OpenAIå…¼å®¹æœåŠ¡å™¨

```bash
# å®‰è£…Ollama
curl -fsSL https://ollama.com/install.sh | sh

# æ‹‰å–æ¨¡å‹
ollama pull llama3.2

# å¯åŠ¨OpenAIå…¼å®¹æœåŠ¡ï¼ˆé»˜è®¤ç«¯å£11434ï¼‰
ollama serve
```

---

## 3. åœ¨UR4Recä¸­ä½¿ç”¨LLM

### 3.1 LLM Generatorï¼ˆç¦»çº¿ç”Ÿæˆç”¨æˆ·åå¥½ï¼‰

```python
from models.llm_generator import LLMPreferenceGenerator

# åˆ›å»ºç”Ÿæˆå™¨ï¼ˆä½¿ç”¨æœ¬åœ°LM Studioï¼‰
generator = LLMPreferenceGenerator(
    llm_backend="openai",
    model_name="local-model",
    api_key="dummy-key",
    base_url="http://localhost:1234/v1"
)

# ç”Ÿæˆç”¨æˆ·åå¥½
item_metadata = {
    101: {"title": "Movie A", "genres": "Action|Adventure"},
    205: {"title": "Movie B", "genres": "Comedy|Drama"}
}

user_pref = generator.generate_user_preference(
    user_id=1,
    user_history=[101, 205],
    item_metadata=item_metadata
)

print(f"User preference: {user_pref}")
```

---

## 4. å®Œæ•´è®­ç»ƒç¤ºä¾‹

### é…ç½®æ–‡ä»¶ (config.yaml)

```yaml
# LLMé…ç½®
llm_backend: openai
llm_model: meta-llama/Llama-2-7b-chat-hf
llm_api_key: dummy-key
llm_base_url: http://localhost:8000/v1  # vLLMåœ°å€

# æ¨¡å‹é…ç½®
embedding_dim: 256
num_heads: 8
# ... å…¶ä»–é…ç½®
```

### è®­ç»ƒè„šæœ¬

```python
import yaml
from models import UR4RecV2

# åŠ è½½é…ç½®
with open('config.yaml') as f:
    config = yaml.safe_load(f)

# åˆ›å»ºæ¨¡å‹
model = UR4RecV2(
    num_items=10000,
    # ... å…¶ä»–å‚æ•°
)

# LLMä»…ç”¨äºç¦»çº¿ç”Ÿæˆç”¨æˆ·åå¥½å’Œç‰©å“æè¿°
# å‚è€ƒ scripts/generate_llm_data.py
```

---

## 5. ç¯å¢ƒå˜é‡é…ç½®

### .envæ–‡ä»¶ç¤ºä¾‹

```bash
# OpenAIå®˜æ–¹
OPENAI_API_KEY=sk-xxx

# Anthropic
ANTHROPIC_API_KEY=sk-ant-xxx

# æœ¬åœ°æœåŠ¡åœ°å€
VLLM_BASE_URL=http://localhost:8000/v1
LM_STUDIO_BASE_URL=http://localhost:1234/v1
OLLAMA_BASE_URL=http://localhost:11434/v1
```

### åœ¨ä»£ç ä¸­ä½¿ç”¨ç¯å¢ƒå˜é‡

```python
import os
from models.llm_generator import LLMPreferenceGenerator

# ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
generator = LLMPreferenceGenerator(
    llm_backend="openai",
    model_name=os.getenv("LLM_MODEL", "gpt-3.5-turbo"),
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("VLLM_BASE_URL")  # å¯é€‰ï¼Œç”¨äºæœ¬åœ°æœåŠ¡
)
```

---

## 6. æ€§èƒ½å¯¹æ¯”

| æœåŠ¡ç±»å‹ | å»¶è¿Ÿ | æˆæœ¬ | éšç§ | æ¨èåœºæ™¯ |
|---------|------|------|------|---------|
| **OpenAI API** | ä½-ä¸­ | æŒ‰é‡è®¡è´¹ | æ•°æ®ä¸Šä¼  | åŸå‹å¼€å‘ã€å°è§„æ¨¡åº”ç”¨ |
| **vLLM (æœ¬åœ°)** | æä½ | ä»…ç¡¬ä»¶ | å®Œå…¨æœ¬åœ° | ç”Ÿäº§ç¯å¢ƒã€å¤§è§„æ¨¡æ¨ç† |
| **LM Studio** | ä½ | å…è´¹ | å®Œå…¨æœ¬åœ° | ä¸ªäººå¼€å‘ã€æµ‹è¯• |
| **Ollama** | ä½ | å…è´¹ | å®Œå…¨æœ¬åœ° | å¿«é€Ÿå®éªŒã€è½»é‡åº”ç”¨ |

---

## 7. æ•…éšœæ’æŸ¥

### é—®é¢˜1: è¿æ¥æœ¬åœ°æœåŠ¡å¤±è´¥

```python
# æµ‹è¯•è¿æ¥
import requests

response = requests.get("http://localhost:8000/v1/models")
print(response.json())
```

**è§£å†³æ–¹æ³•**:
- ç¡®è®¤æœåŠ¡å·²å¯åŠ¨
- æ£€æŸ¥ç«¯å£å·æ˜¯å¦æ­£ç¡®
- ç¡®è®¤é˜²ç«å¢™è®¾ç½®

### é—®é¢˜2: APIå¯†é’¥é”™è¯¯

å¯¹äºæœ¬åœ°æœåŠ¡ï¼Œé€šå¸¸ä¸éœ€è¦çœŸå®APIå¯†é’¥ï¼š

```python
generator = LLMPreferenceGenerator(
    llm_backend="openai",
    model_name="local-model",
    api_key="dummy-key",  # ä»»æ„å­—ç¬¦ä¸²
    base_url="http://localhost:8000/v1"
)
```

### é—®é¢˜3: æ¨¡å‹ä¸å­˜åœ¨

```python
# åˆ—å‡ºå¯ç”¨æ¨¡å‹
response = requests.get("http://localhost:8000/v1/models")
models = response.json()
print("Available models:", models)
```

---

## 8. æœ€ä½³å®è·µ

### 8.1 å¼€å‘ç¯å¢ƒ
- ä½¿ç”¨**LM Studio**æˆ–**Ollama**è¿›è¡Œå¿«é€ŸåŸå‹å¼€å‘
- æ¨¡å‹æ¨è: `llama3.2`, `qwen2.5`, `gemma2`

### 8.2 ç”Ÿäº§ç¯å¢ƒ
- ä½¿ç”¨**vLLM**éƒ¨ç½²ï¼Œæ€§èƒ½æœ€ä¼˜
- å¯ç”¨æ‰¹å¤„ç†å’Œè¿ç»­æ‰¹å¤„ç†(continuous batching)
- ä½¿ç”¨GPUåŠ é€Ÿ

### 8.3 æˆæœ¬ä¼˜åŒ–
- ç¼“å­˜å¸¸è§æŸ¥è¯¢ç»“æœï¼ˆå·²åœ¨LLMPreferenceGeneratorä¸­å®ç°ï¼‰
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹(7Bè€Œé70B)å¤„ç†ç®€å•ä»»åŠ¡
- æ‰¹é‡å¤„ç†è¯·æ±‚

### 8.4 å®‰å…¨æ€§
- æœ¬åœ°éƒ¨ç½²é¿å…æ•°æ®æ³„éœ²
- ä½¿ç”¨APIå¯†é’¥ç®¡ç†å·¥å…·(å¦‚Vault)
- é™åˆ¶APIè®¿é—®é¢‘ç‡

---

## 9. å¿«é€Ÿæµ‹è¯•

### æµ‹è¯•è„šæœ¬

```python
# test_llm_api.py
from models.llm_generator import LLMPreferenceGenerator

def test_llm_connection(base_url=None):
    """æµ‹è¯•LLMè¿æ¥"""
    try:
        generator = LLMPreferenceGenerator(
            llm_backend="openai",
            model_name="gpt-3.5-turbo" if not base_url else "local-model",
            api_key="dummy-key",
            base_url=base_url
        )

        # æµ‹è¯•ç®€å•ç”Ÿæˆ
        test_metadata = {
            1: {"title": "Test Item", "genres": "Test"}
        }
        result = generator.generate_user_preference(
            user_id=1,
            user_history=[1],
            item_metadata=test_metadata
        )

        print(f"âœ“ Connection successful!")
        print(f"Response: {result[:100]}...")
        return True

    except Exception as e:
        print(f"âœ— Connection failed: {e}")
        return False

# æµ‹è¯•ä¸åŒæœåŠ¡
print("Testing vLLM...")
test_llm_connection("http://localhost:8000/v1")

print("\nTesting LM Studio...")
test_llm_connection("http://localhost:1234/v1")
```

è¿è¡Œæµ‹è¯•:
```bash
source venv/bin/activate
python test_llm_api.py
```

---

## 10. å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- [OpenAI APIæ–‡æ¡£](https://platform.openai.com/docs)
- [vLLMæ–‡æ¡£](https://docs.vllm.ai)
- [LM Studio](https://lmstudio.ai)
- [Ollamaæ–‡æ¡£](https://ollama.com/docs)

### æ¨¡å‹é€‰æ‹©
- **å¿«é€ŸåŸå‹**: `llama3.2` (3B), `gemma2` (2B)
- **å¹³è¡¡æ€§èƒ½**: `llama3.2` (7B), `qwen2.5` (7B)
- **æœ€ä½³è´¨é‡**: `llama3.2` (70B), `qwen2.5` (72B)

---

## æ€»ç»“

UR4Recç°åœ¨å®Œå…¨æ”¯æŒï¼š
- âœ… OpenAIå®˜æ–¹API
- âœ… æ‰€æœ‰OpenAIå…¼å®¹API (vLLM, LocalAI, LM Studio, Ollamaç­‰)
- âœ… Anthropic Claude API
- âœ… æœ¬åœ°Transformersæ¨¡å‹

**æ¨èé…ç½®**:
- å¼€å‘: LM Studio + llama3.2
- ç”Ÿäº§: vLLM + llama3.2 (7B)
- å®éªŒ: OpenAI API (gpt-3.5-turbo)
