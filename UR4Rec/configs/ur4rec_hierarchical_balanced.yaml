# UR4Rec Hierarchical MoE + CLIP - 平衡版本
# 在性能提升和训练效率之间取得平衡
# 目标: HR@10从0.4提升至0.55-0.65（第一阶段目标）

# Dataset settings
num_items: 1682
max_seq_len: 50

# ========================================
# SASRec - 适度增强（避免过拟合）
# ========================================
sasrec_hidden_dim: 512  # 保持原始512维（数据集较小，768维容易过拟合）
sasrec_num_blocks: 4    # 从3增加到4（适度增加深度）
sasrec_num_heads: 8     # 保持原始8头
sasrec_dropout: 0.3     # 增加dropout防止过拟合

# Text encoder parameters
text_model_name: "all-MiniLM-L6-v2"
text_embedding_dim: 384
retriever_output_dim: 512  # 匹配SASRec维度

# ========================================
# Hierarchical MoE - 核心改进保持不变
# ========================================
use_hierarchical_moe: true
use_clip: true
clip_features_path: "UR4Rec/data/clip_features.pt"

# MoE架构参数
moe_num_heads: 8
moe_dropout: 0.1
moe_num_proxies: 8
num_sub_experts: 3  # 每个模态3个sub-experts (共9个experts)

# Sub-expert specialization
sub_expert_specialization:
  user_pref:
    - "genre_preference"
    - "mood_preference"
    - "style_preference"
  item_desc:
    - "content_understanding"
    - "thematic_analysis"
    - "quality_assessment"
  image_feat:
    - "visual_composition"
    - "color_texture"
    - "object_recognition"

# Memory mechanism
max_memory_size: 20
update_trigger: "INTERACTION_COUNT"
interaction_threshold: 5
drift_threshold: 0.3
decay_factor: 0.95

# Fusion parameters
fusion_method: "weighted"
sasrec_weight: 0.4
retriever_weight: 0.6

# ========================================
# 负采样 - 平衡难度和训练速度
# ========================================
num_negatives: 100  # 从20增加到100（5x提升，而非25x）
                     # 覆盖率: 100/1682 = 6% (vs 原始1.2%)

# ========================================
# Training parameters - 优化学习率
# ========================================
sasrec_lr: 0.0005    # 适度提高学习率加快收敛
retriever_lr: 0.001  # 适度提高
use_uncertainty_weighting: false
use_adaptive_alternating: false

# Training stages - 减少每阶段epochs加快迭代
stages:
  - pretrain_sasrec      # 阶段1: 预训练SASRec
  - pretrain_retriever   # 阶段2: 预训练Hierarchical MoE Retriever
  - joint_finetune       # 阶段3: 联合微调
  - end_to_end          # 阶段4: 端到端训练

epochs_per_stage: 25  # 从50减少到25（先快速验证效果）
patience: 15          # 从25减少到15

# Other settings
batch_size: 16  # 恢复到原始16（减少内存压力）
num_workers: 4
seed: 42

# ========================================
# 预期效果（保守估计）
# ========================================
# 1. 适度增强SASRec (4层512维): +2-3%
# 2. Hierarchical MoE (9 experts): +8-12%
# 3. 100个负样本: +3-5%
# 总计: +13-20% → HR@10: 0.40 → 0.45-0.48 (第一阶段)
#
# 模型参数: ~45M (vs 71M激进版, 30M基线版)
# 训练时间: 预计4-6小时/stage (CPU)
