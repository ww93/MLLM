#!/usr/bin/env python3
"""
ä»LLMç”Ÿæˆçš„itemæè¿°æå–æ–‡æœ¬åµŒå…¥ç‰¹å¾

è¿™ä¸ªè„šæœ¬ä» data/llm_generated/item_descriptions.json è¯»å–itemæè¿°ï¼Œ
ä½¿ç”¨Sentence-BERTæ¨¡å‹æå–æ–‡æœ¬åµŒå…¥ï¼Œä¿å­˜ä¸º .pt æ–‡ä»¶ä¾›FedMemè®­ç»ƒä½¿ç”¨ã€‚

ç”¨æ³•:
    python UR4Rec/scripts/generate_text_features.py

è¾“å‡º:
    UR4Rec/data/item_text_features.pt - [num_items, 384] æ–‡æœ¬ç‰¹å¾tensor
"""

import json
import torch
import os
from typing import Dict
from sentence_transformers import SentenceTransformer


def load_item_descriptions(json_path: str) -> Dict[int, str]:
    """
    ä»JSONæ–‡ä»¶åŠ è½½itemæè¿°

    Args:
        json_path: JSONæ–‡ä»¶è·¯å¾„

    Returns:
        å­—å…¸ {item_id: description_text}
    """
    print(f"ğŸ“– æ­£åœ¨åŠ è½½itemæè¿°: {json_path}")
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # å°†å­—ç¬¦ä¸²é”®è½¬æ¢ä¸ºæ•´æ•°
    descriptions = {int(k): v for k, v in data.items()}
    print(f"âœ… åŠ è½½äº† {len(descriptions)} ä¸ªitemæè¿°")
    return descriptions


def generate_text_embeddings(
    descriptions: Dict[int, str],
    model_name: str = 'all-MiniLM-L6-v2',
    device: str = 'cpu'
) -> torch.Tensor:
    """
    ä½¿ç”¨Sentence-BERTç”Ÿæˆæ–‡æœ¬åµŒå…¥

    Args:
        descriptions: {item_id: description_text}
        model_name: Sentence-BERTæ¨¡å‹åç§°
        device: 'cpu', 'cuda', æˆ– 'mps'

    Returns:
        æ–‡æœ¬ç‰¹å¾tensor [num_items, embedding_dim]
    """
    print(f"\nğŸ¤– æ­£åœ¨åŠ è½½Sentence-BERTæ¨¡å‹: {model_name}")
    model = SentenceTransformer(model_name, device=device)
    embedding_dim = model.get_sentence_embedding_dimension()
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ŒåµŒå…¥ç»´åº¦: {embedding_dim}")

    # ç¡®å®šæœ€å¤§item_idä»¥åˆ›å»ºæ­£ç¡®å¤§å°çš„tensor
    max_item_id = max(descriptions.keys())
    num_items = max_item_id + 1

    print(f"\nğŸ”¢ Item IDèŒƒå›´: 1 - {max_item_id}")
    print(f"ğŸ“¦ åˆ›å»ºç‰¹å¾çŸ©é˜µ: [{num_items}, {embedding_dim}]")

    # åˆå§‹åŒ–ç‰¹å¾çŸ©é˜µï¼ˆé›¶å¡«å……ï¼‰
    text_features = torch.zeros(num_items, embedding_dim, dtype=torch.float32)

    # å‡†å¤‡æ‰¹é‡ç¼–ç çš„æ–‡æœ¬å’Œå¯¹åº”çš„item_id
    item_ids = []
    texts = []
    for item_id in sorted(descriptions.keys()):
        item_ids.append(item_id)
        texts.append(descriptions[item_id])

    print(f"\nğŸš€ æ­£åœ¨ç”Ÿæˆ {len(texts)} ä¸ªitemçš„æ–‡æœ¬åµŒå…¥...")

    # æ‰¹é‡ç¼–ç ï¼ˆæ›´é«˜æ•ˆï¼‰
    batch_size = 32
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        batch_ids = item_ids[i:i+batch_size]

        # ç”ŸæˆåµŒå…¥
        embeddings = model.encode(
            batch_texts,
            convert_to_tensor=True,
            show_progress_bar=False,
            device=device
        )

        # å¡«å……åˆ°ç‰¹å¾çŸ©é˜µ
        for j, item_id in enumerate(batch_ids):
            text_features[item_id] = embeddings[j].cpu()

        if (i // batch_size + 1) % 10 == 0:
            print(f"  è¿›åº¦: {i + len(batch_texts)}/{len(texts)} items")

    print(f"âœ… æ–‡æœ¬åµŒå…¥ç”Ÿæˆå®Œæˆï¼")

    # ç»Ÿè®¡ä¿¡æ¯
    num_nonzero = (text_features.abs().sum(dim=1) > 0).sum().item()
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  - æœ‰æ•ˆç‰¹å¾: {num_nonzero}/{num_items}")
    print(f"  - é›¶å¡«å……: {num_items - num_nonzero}")
    print(f"  - ç‰¹å¾å½¢çŠ¶: {text_features.shape}")
    print(f"  - ç‰¹å¾èŒƒå›´: [{text_features.min():.4f}, {text_features.max():.4f}]")

    return text_features


def main():
    # è·¯å¾„é…ç½®
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    json_path = os.path.join(project_root, 'data', 'llm_generated', 'item_descriptions.json')
    output_path = os.path.join(project_root, 'UR4Rec', 'data', 'item_text_features.pt')

    print("=" * 70)
    print("ğŸ“ ä»LLMç”Ÿæˆçš„æè¿°æå–æ–‡æœ¬ç‰¹å¾")
    print("=" * 70)

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°itemæè¿°æ–‡ä»¶: {json_path}")

    # åŠ è½½æè¿°
    descriptions = load_item_descriptions(json_path)

    # é€‰æ‹©è®¾å¤‡
    if torch.cuda.is_available():
        device = 'cuda'
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = 'mps'
    else:
        device = 'cpu'
    print(f"\nğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device.upper()}")

    # ç”ŸæˆåµŒå…¥
    text_features = generate_text_embeddings(descriptions, device=device)

    # ä¿å­˜ç‰¹å¾
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ç‰¹å¾åˆ°: {output_path}")
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    torch.save(text_features, output_path)

    # éªŒè¯ä¿å­˜
    file_size_mb = os.path.getsize(output_path) / (1024 * 1024)
    print(f"âœ… ä¿å­˜æˆåŠŸï¼æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB")

    # æµ‹è¯•åŠ è½½
    print(f"\nğŸ§ª éªŒè¯æ–‡ä»¶å¯ä»¥æ­£ç¡®åŠ è½½...")
    loaded_features = torch.load(output_path)
    print(f"âœ… åŠ è½½æˆåŠŸï¼å½¢çŠ¶: {loaded_features.shape}")

    print("\n" + "=" * 70)
    print("ğŸ‰ æ–‡æœ¬ç‰¹å¾ç”Ÿæˆå®Œæˆï¼")
    print("=" * 70)
    print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"   python UR4Rec/scripts/train_fedmem.py \\")
    print(f"       --visual_file clip_features.pt \\")
    print(f"       --text_file item_text_features.pt \\")
    print(f"       --contrastive_lambda 0.1")
    print()


if __name__ == "__main__":
    main()
