"""
FedMem Client: Federated Learning Client with Local Dynamic Memory

å¸¦æœ¬åœ°åŠ¨æ€è®°å¿†çš„è”é‚¦å­¦ä¹ å®¢æˆ·ç«¯
- é›†æˆLocalDynamicMemory
- Surprise-basedè®°å¿†æ›´æ–°
- Memory Prototypesæå–ä¸èšåˆ
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import copy
import numpy as np
from typing import Dict, List, OrderedDict, Optional, Tuple
from collections import defaultdict

from .local_dynamic_memory import LocalDynamicMemory
from .federated_aggregator import FederatedAggregator


class ClientDataset(Dataset):
    """
    å®¢æˆ·ç«¯æ•°æ®é›†ï¼ˆå¤ç”¨ä¹‹å‰çš„å®ç°ï¼‰
    ä½¿ç”¨ leave-one-out åˆ’åˆ†
    """

    def __init__(
        self,
        user_id: int,
        sequence: List[int],
        max_seq_len: int = 50,
        split: str = "train"
    ):
        self.user_id = user_id
        self.full_sequence = sequence
        self.max_seq_len = max_seq_len
        self.split = split

        # Leave-one-out åˆ’åˆ†
        if split == "test":
            self.target_item = sequence[-1]
            self.input_seq = sequence[:-1]
            self.train_samples = None
        elif split == "val":
            if len(sequence) < 2:
                self.target_item = sequence[-1]
                self.input_seq = sequence[:-1]
            else:
                self.target_item = sequence[-2]
                self.input_seq = sequence[:-2]
            self.train_samples = None
        else:  # train
            if len(sequence) < 3:
                self.target_item = sequence[-1]
                self.input_seq = sequence[:-1]
                self.train_samples = None
            else:
                # æ»‘åŠ¨çª—å£ç”Ÿæˆè®­ç»ƒæ ·æœ¬
                train_seq = sequence[:-2]
                if len(train_seq) <= 1:
                    self.target_item = sequence[-1]
                    self.input_seq = sequence[:-1]
                    self.train_samples = None
                else:
                    self.train_samples = []
                    for i in range(1, len(train_seq)):
                        input_items = train_seq[:i]
                        target = train_seq[i]
                        self.train_samples.append((input_items, target))
                    self.target_item = None
                    self.input_seq = None

    def __len__(self) -> int:
        if self.split == 'train' and self.train_samples is not None:
            return len(self.train_samples)
        return 1

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        if self.split == 'train' and self.train_samples is not None:
            input_items, target_item = self.train_samples[idx]
        else:
            input_items = self.input_seq
            target_item = self.target_item

        # æˆªæ–­/å¡«å……åºåˆ—
        if len(input_items) > self.max_seq_len:
            input_items = input_items[-self.max_seq_len:]
        else:
            padding = [0] * (self.max_seq_len - len(input_items))
            input_items = padding + input_items

        return {
            'user_id': torch.tensor(self.user_id, dtype=torch.long),
            'item_seq': torch.tensor(input_items, dtype=torch.long),
            'target_item': torch.tensor(target_item, dtype=torch.long)
        }


class FedMemClient:
    """
    FedMemè”é‚¦å­¦ä¹ å®¢æˆ·ç«¯

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. ç»´æŠ¤æœ¬åœ°åŠ¨æ€è®°å¿†ï¼ˆLocalDynamicMemoryï¼‰
    2. è®­ç»ƒæ—¶ä½¿ç”¨Surpriseæœºåˆ¶æ›´æ–°è®°å¿†
    3. ä¸Šä¼ æ¨¡å‹å‚æ•° + Memory Prototypes
    4. æ¥æ”¶å…¨å±€æ¨¡å‹ + Global Abstract Memory
    """

    def __init__(
        self,
        client_id: int,
        model: nn.Module,
        user_sequence: List[int],
        device: str = 'cpu',
        # [NEW] å¤šæ¨¡æ€ç‰¹å¾
        item_visual_feats: Optional[torch.Tensor] = None,
        item_text_feats: Optional[torch.Tensor] = None,
        # è®­ç»ƒå‚æ•°
        learning_rate: float = 1e-3,
        weight_decay: float = 1e-5,
        local_epochs: int = 1,
        batch_size: int = 32,
        max_seq_len: int = 50,
        # è´Ÿé‡‡æ ·
        num_negatives: int = 100,
        num_items: int = 1682,
        # è®°å¿†å‚æ•° (Two-tier: ST + LT)
        memory_capacity: int = 200,         # LT (long-term) å®¹é‡ (æ¨è200 for ML-1M)
        surprise_threshold: float = 0.5,    # å…¼å®¹å‚æ•°ï¼Œæ–°ç‰ˆæœ¬ä¸»è¦ä½¿ç”¨novelty
        contrastive_lambda: float = 0.1,
        num_memory_prototypes: int = 5,
        # è´Ÿé‡‡æ ·è¯„ä¼°å‚æ•°
        use_negative_sampling: bool = False,
        num_negatives_eval: int = 100
    ):
        """
        Args:
            client_id: å®¢æˆ·ç«¯IDï¼ˆå¯¹åº”user_idï¼‰
            model: å…¨å±€æ¨¡å‹ï¼ˆUR4RecV2MoEï¼‰
            user_sequence: ç”¨æˆ·äº¤äº’åºåˆ—
            device: è®¡ç®—è®¾å¤‡
            item_visual_feats: [NEW] ç‰©å“è§†è§‰ç‰¹å¾ [num_items, img_dim]
            item_text_feats: [NEW] ç‰©å“æ–‡æœ¬ç‰¹å¾ [num_items, text_dim]
            learning_rate: å­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡
            local_epochs: æœ¬åœ°è®­ç»ƒè½®æ•°
            batch_size: æ‰¹å¤§å°
            max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
            num_negatives: è´Ÿæ ·æœ¬æ•°é‡
            num_items: ç‰©å“æ€»æ•°
            memory_capacity: LT (long-term) è®°å¿†å®¹é‡ï¼Œæ¨è200 (ML-1M)
            surprise_threshold: å…¼å®¹å‚æ•°ï¼Œæ–°ç‰ˆæœ¬ä¸»è¦ä½¿ç”¨novelty-basedå†™å…¥
            contrastive_lambda: å¯¹æ¯”å­¦ä¹ æŸå¤±æƒé‡
            num_memory_prototypes: è®°å¿†åŸå‹æ•°é‡ï¼ˆä»LTæå–ï¼‰
        """
        self.client_id = client_id
        self.device = device
        self.local_epochs = local_epochs
        self.batch_size = batch_size
        self.max_seq_len = max_seq_len
        self.num_negatives = num_negatives
        self.num_items = num_items
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        self.contrastive_lambda = contrastive_lambda
        self.num_memory_prototypes = num_memory_prototypes
        # è°ƒè¯•å¼€å…³ï¼šé»˜è®¤å…³é—­ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡ FEDMEM_DEBUG=1 æ‰“å¼€
        self._debug = bool(int(os.environ.get('FEDMEM_DEBUG', '0')))


        # [NEW] å­˜å‚¨å¤šæ¨¡æ€ç‰¹å¾
        self.item_visual_feats = item_visual_feats
        self.item_text_feats = item_text_feats

        # [FIX 3] å®Œæ•´æ€§æ£€æŸ¥ï¼šéªŒè¯å¤šæ¨¡æ€ç‰¹å¾æ˜¯å¦æ­£ç¡®åŠ è½½
        if getattr(self, '_debug', False) and client_id == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯æ‰“å°ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
            print(f"\n[FIX 3] å®¢æˆ·ç«¯ {client_id} å¤šæ¨¡æ€ç‰¹å¾å®Œæ•´æ€§æ£€æŸ¥:")
            if self.item_visual_feats is not None:
                print(f"  âœ“ è§†è§‰ç‰¹å¾å·²åŠ è½½: shape={self.item_visual_feats.shape}, "
                      f"dtype={self.item_visual_feats.dtype}, device={self.item_visual_feats.device}")
                print(f"    ç»Ÿè®¡: min={self.item_visual_feats.min():.4f}, "
                      f"max={self.item_visual_feats.max():.4f}, mean={self.item_visual_feats.mean():.4f}")
            else:
                print(f"  âœ— è§†è§‰ç‰¹å¾æœªåŠ è½½ (item_visual_feats=None)")

            if self.item_text_feats is not None:
                print(f"  âœ“ æ–‡æœ¬ç‰¹å¾å·²åŠ è½½: shape={self.item_text_feats.shape}, "
                      f"dtype={self.item_text_feats.dtype}, device={self.item_text_feats.device}")
                print(f"    ç»Ÿè®¡: min={self.item_text_feats.min():.4f}, "
                      f"max={self.item_text_feats.max():.4f}, mean={self.item_text_feats.mean():.4f}")
            else:
                print(f"  âœ— æ–‡æœ¬ç‰¹å¾æœªåŠ è½½ (item_text_feats=None)")

            if self.item_visual_feats is None and self.item_text_feats is None:
                print(f"  âš ï¸  è­¦å‘Š: æœªåŠ è½½ä»»ä½•å¤šæ¨¡æ€ç‰¹å¾ï¼æ¨¡å‹å°†ä»…ä½¿ç”¨IDåµŒå…¥ã€‚")

        # è´Ÿé‡‡æ ·è¯„ä¼°å‚æ•°
        self.use_negative_sampling = use_negative_sampling
        self.num_negatives_eval = num_negatives_eval

        # å»¶è¿Ÿæ¨¡å‹å®ä¾‹åŒ–
        self._model_reference = model
        self.model = None
        self.optimizer = None

        # æœ¬åœ°æ•°æ®
        self.user_sequence = user_sequence
        # [Critical Fix] ç¼“å­˜ç”¨æˆ·å†å²äº¤äº’é›†åˆï¼Œç”¨äºè´Ÿé‡‡æ ·æ—¶æ’é™¤
        self.user_items = set(user_sequence)  # å¿«é€ŸæŸ¥æ‰¾O(1)
        self.train_dataset = ClientDataset(
            client_id, user_sequence, max_seq_len, split="train"
        )
        self.val_dataset = ClientDataset(
            client_id, user_sequence, max_seq_len, split="val"
        )
        self.test_dataset = ClientDataset(
            client_id, user_sequence, max_seq_len, split="test"
        )

        # ç”¨äºè®¡ç®—è®­ç»ƒæƒé‡
        self.num_train_samples = len(self.train_dataset)

        # ã€FedMemæ ¸å¿ƒã€‘åˆå§‹åŒ–æœ¬åœ°åŠ¨æ€è®°å¿† (Two-tier: ST + LT)
        # - ST (short-term): FIFO, capacity=50, æ•è·æœ€è¿‘å…´è¶£
        # - LT (long-term): novelty-gated, capacity=memory_capacity, ç¨³å®šå¤šæ ·æ€§å­˜å‚¨

        # [FIX] æ¨æ–­ç‰¹å¾ç»´åº¦ï¼Œç”¨äºempty memoryæ—¶è¿”å›æ­£ç¡®å½¢çŠ¶çš„é›¶å¼ é‡
        id_emb_dim = getattr(model, 'sasrec_hidden_dim', 128)  # ä»æ¨¡å‹è·å–IDåµŒå…¥ç»´åº¦
        visual_emb_dim = item_visual_feats.shape[1] if item_visual_feats is not None else 512
        text_emb_dim = item_text_feats.shape[1] if item_text_feats is not None else 384

        self.local_memory = LocalDynamicMemory(
            capacity=memory_capacity,           # LTå®¹é‡ (æ¨è200)
            surprise_threshold=surprise_threshold,  # å…¼å®¹å‚æ•°
            device=device,
            # [FIX] ä¼ å…¥ç‰¹å¾ç»´åº¦ï¼Œç¡®ä¿empty memoryæ—¶è¿”å›æ­£ç¡®å½¢çŠ¶
            id_emb_dim=id_emb_dim,
            visual_emb_dim=visual_emb_dim,
            text_emb_dim=text_emb_dim
            # å…¶ä»–å‚æ•°ä½¿ç”¨æ•°æ®é©±åŠ¨çš„é»˜è®¤å€¼ (è§local_dynamic_memory.py)
        )

    def _ensure_model_initialized(self):
        """ç¡®ä¿æ¨¡å‹å·²åˆå§‹åŒ–ï¼ˆå»¶è¿Ÿå®ä¾‹åŒ–ï¼‰"""
        if self.model is None:
            self.model = copy.deepcopy(self._model_reference).to(self.device)

            # [æ–¹æ¡ˆ2è°ƒè¯•] éªŒè¯å®¢æˆ·ç«¯æ¨¡å‹çš„ç»´åº¦
            if self.client_id == 0 and hasattr(self.model, 'visual_expert'):
                print(f"\n[æ–¹æ¡ˆ2è°ƒè¯•] å®¢æˆ·ç«¯ {self.client_id} æ¨¡å‹ç»´åº¦éªŒè¯:")
                print(f"  preserve_multimodal_dim: {self.model.preserve_multimodal_dim}")
                print(f"  visual_expert.output_dim: {self.model.visual_expert.output_dim}")
                print(f"  semantic_expert.output_dim: {self.model.semantic_expert.output_dim}")
                print(f"  vis_layernorm.normalized_shape: {self.model.vis_layernorm.normalized_shape}")
                print(f"  sem_layernorm.normalized_shape: {self.model.sem_layernorm.normalized_shape}")

            # [ä¼˜åŒ–4] å†»ç»“embeddingsåï¼Œåªä¼˜åŒ–requires_grad=Trueçš„å‚æ•°
            trainable_params = [p for p in self.model.parameters() if p.requires_grad]
            self.optimizer = optim.Adam(
                trainable_params,
                lr=self.learning_rate,
                weight_decay=self.weight_decay
            )
            # [åŠ é€Ÿä¼˜åŒ–1] åˆå§‹åŒ–æ··åˆç²¾åº¦è®­ç»ƒçš„GradScaler
            # å…¼å®¹å­—ç¬¦ä¸²å’Œtorch.deviceå¯¹è±¡
            device_type = self.device if isinstance(self.device, str) else self.device.type
            if device_type == 'cuda':
                self.scaler = torch.cuda.amp.GradScaler()
            else:
                self.scaler = None

    def freeze_embeddings_for_alignment(self):
        """
        [æ–°ç­–ç•¥] å†»ç»“ID Embeddingï¼Œè®­ç»ƒå¤šæ¨¡æ€æŠ•å½±å±‚ä»¥å¯¹é½åˆ°IDç©ºé—´

        æ ¸å¿ƒæ€æƒ³:
        - é¢„è®­ç»ƒçš„ID embeddingå·²ç»å­¦åˆ°äº†è‰¯å¥½çš„ç‰©å“è¡¨ç¤ºç©ºé—´
        - å†»ç»“ID embeddingï¼Œé˜²æ­¢å¤šæ¨¡æ€ç‰¹å¾ç ´åè¿™ä¸ªç©ºé—´
        - è®­ç»ƒvisual_projå’Œtext_projï¼Œè®©å¤šæ¨¡æ€ç‰¹å¾å¯¹é½åˆ°IDç©ºé—´

        å†»ç»“ç­–ç•¥:
        - å†»ç»“: item_embedding, positional_embedding (ä¿æŒIDç©ºé—´ç¨³å®š)
        - ä¿æŒå¯è®­ç»ƒ: Transformer blocks, visual_proj, text_proj, Router, Experts

        é€‚ç”¨åœºæ™¯: æœ‰é«˜è´¨é‡çš„é¢„è®­ç»ƒID embeddingæ—¶ä½¿ç”¨

        è°ƒç”¨æ—¶æœº: åœ¨åŠ è½½é¢„è®­ç»ƒæƒé‡åç«‹å³è°ƒç”¨
        """
        self._ensure_model_initialized()

        frozen_params = []
        trainable_params = []

        for name, param in self.model.named_parameters():
            # åªå†»ç»“embeddingå±‚ï¼ˆIDç©ºé—´ï¼‰
            if 'item_emb' in name.lower() or 'positional_emb' in name.lower():
                param.requires_grad = False
                frozen_params.append(name)
            else:
                # å…¶ä»–å±‚å…¨éƒ¨ä¿æŒå¯è®­ç»ƒï¼ˆåŒ…æ‹¬æŠ•å½±å±‚ã€Transformerã€Routerã€Expertsï¼‰
                param.requires_grad = True
                trainable_params.append(name)

        # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆåªåŒ…å«å¯è®­ç»ƒå‚æ•°ï¼‰
        trainable_params_list = [p for p in self.model.parameters() if p.requires_grad]
        self.optimizer = optim.Adam(
            trainable_params_list,
            lr=self.learning_rate,
            weight_decay=self.weight_decay
        )

        print(f"[å¯¹é½ç­–ç•¥] å®¢æˆ·ç«¯ {self.client_id} - å†»ç»“ID Embeddingï¼Œè®­ç»ƒæŠ•å½±å±‚:")
        print(f"  â„ï¸  å†»ç»“å‚æ•°æ•°: {len(frozen_params)}")
        print(f"  ğŸ”¥ å¯è®­ç»ƒå‚æ•°æ•°: {len(trainable_params)}")
        if len(frozen_params) > 0:
            print(f"  å†»ç»“å±‚: {', '.join(frozen_params[:5])}")

        # ç»Ÿè®¡å¯è®­ç»ƒçš„æŠ•å½±å±‚å‚æ•°
        proj_params = [name for name in trainable_params if 'proj' in name.lower()]
        if proj_params:
            print(f"  âœ“ æŠ•å½±å±‚å¯è®­ç»ƒ: {len(proj_params)}ä¸ª (ç”¨äºå¯¹é½åˆ°IDç©ºé—´)")

    def freeze_embeddings(self):
        """
        [å·²åºŸå¼ƒ] å®Œå…¨å†»ç»“embeddingå±‚

        æ³¨æ„: æ­¤æ–¹æ³•å·²è¢« freeze_embeddings_for_alignment() æ›¿ä»£
        æ–°æ–¹æ³•å…è®¸æŠ•å½±å±‚è®­ç»ƒï¼Œæ•ˆæœæ›´å¥½
        """
        # å…¼å®¹æ€§ä¿ç•™ï¼Œè°ƒç”¨æ–°æ–¹æ³•
        self.freeze_embeddings_for_alignment()

    def release_model(self):
        """é‡Šæ”¾æ¨¡å‹å†…å­˜"""
        if self.model is not None:
            del self.model
            del self.optimizer
            self.model = None
            self.optimizer = None
            torch.cuda.empty_cache()

    def get_data_size(self) -> int:
        """è·å–å®¢æˆ·ç«¯è®­ç»ƒæ•°æ®é‡"""
        return self.num_train_samples

    def set_model_parameters(self, global_parameters: OrderedDict) -> None:
        """
        ä»æœåŠ¡å™¨æ¥æ”¶å…¨å±€æ¨¡å‹å‚æ•°

        Args:
            global_parameters: å…¨å±€æ¨¡å‹å‚æ•°
        """
        self._ensure_model_initialized()
        self.model.load_state_dict(global_parameters, strict=True)

    def get_model_parameters(self) -> OrderedDict:
        """
        ä¸Šä¼ æœ¬åœ°æ¨¡å‹å‚æ•°åˆ°æœåŠ¡å™¨

        Returns:
            æœ¬åœ°æ¨¡å‹å‚æ•°
        """
        self._ensure_model_initialized()
        return FederatedAggregator.get_model_parameters(self.model)

    def get_memory_prototypes(self) -> Optional[torch.Tensor]:
        """
        ã€FedMemæ ¸å¿ƒã€‘æå–è®°å¿†åŸå‹

        Returns:
            [K, emb_dim] è®°å¿†åŸå‹çŸ©é˜µ
        """
        return self.local_memory.get_memory_prototypes(k=self.num_memory_prototypes)

    def set_global_abstract_memory(self, global_prototypes: torch.Tensor):
        """
        ã€FedMemæ ¸å¿ƒã€‘æ¥æ”¶å…¨å±€æŠ½è±¡è®°å¿†

        Args:
            global_prototypes: [K, emb_dim] å…¨å±€åŸå‹åµŒå…¥
        """
        self.local_memory.set_global_abstract_memory(global_prototypes)

    def train_local_model(
        self,
        verbose: bool = False
    ) -> Dict[str, float]:
        """
        åœ¨æœ¬åœ°æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶æ›´æ–°åŠ¨æ€è®°å¿†ã€‚

        è®­ç»ƒé‡‡ç”¨ **æ˜¾å¼è´Ÿé‡‡æ ·**ï¼ˆä¸ç»å…¸ SASRec / NCF è¯„ä¼°åè®®ä¸€è‡´ï¼‰ï¼š
        - å¯¹æ¯ä¸ªæ­£æ ·æœ¬é‡‡æ · num_negatives ä¸ªè´Ÿæ ·æœ¬ï¼Œæ„é€ å€™é€‰é›† [pos + negs]
        - logits: [B, 1+N]ï¼Œæ ‡ç­¾æ’ä¸º 0
        - è®­ç»ƒæ—¶å¯é€‰åŠ å…¥â€œæ¨¡æ€å¯¹é½æŸå¤±â€ï¼ˆStage 2/3ï¼‰ï¼šè®©å¤šæ¨¡æ€èåˆè¡¨ç¤ºå¯¹é½å†»ç»“çš„ ID embedding ç©ºé—´

        Args:
            verbose: æ˜¯å¦æ‰“å°è®­ç»ƒä¿¡æ¯ï¼ˆé»˜è®¤å…³é—­ï¼›å»ºè®®ä»… client_id==0 æ‰“å¼€ï¼‰

        Returns:
            dict: è®­ç»ƒæŒ‡æ ‡
        """
        self._ensure_model_initialized()
        self.model.train()

        train_loader = DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True
        )

        total_rec_loss = 0.0
        total_align_loss = 0.0
        num_batches = 0

        for _ in range(self.local_epochs):
            epoch_rec_loss = 0.0
            epoch_align_loss = 0.0

            for batch in train_loader:
                user_ids = batch['user_id'].tolist()
                item_seqs = batch['item_seq'].to(self.device)
                target_items = batch['target_item'].to(self.device)  # [B]
                bsz = target_items.size(0)

                # 1) è´Ÿé‡‡æ ·ï¼šæ„é€ å€™é€‰é›† [B, 1+N]
                neg_items = self._negative_sampling(batch_size=bsz, target_items=target_items)  # [B, N]
                candidate_items = torch.cat([target_items.unsqueeze(1), neg_items], dim=1)      # [B, 1+N]
                labels = torch.zeros(bsz, dtype=torch.long, device=self.device)                # æ­£æ ·æœ¬æ’åœ¨ç¬¬0åˆ—

                # 2) è®°å¿†æ£€ç´¢ï¼ˆå¯ä¸ºç©ºï¼‰
                memory_visual, memory_text = self._retrieve_multimodal_memory_batch(
                    batch_size=bsz,
                    top_k=20
                )

                # 3) å€™é€‰å¤šæ¨¡æ€ç‰¹å¾ï¼ˆå¯ä¸ºç©ºï¼‰
                cand_visual = self._get_candidate_visual_features(candidate_items)
                cand_text = self._get_candidate_text_features(candidate_items)

                # 4) å‰å‘ + æŸå¤±
                self.optimizer.zero_grad()

                with torch.amp.autocast('cuda', enabled=(self.scaler is not None)):
                    logits, info = self.model(
                        user_ids=user_ids,
                        input_seq=item_seqs,
                        target_items=candidate_items,   # [B, 1+N]
                        memory_visual=memory_visual,
                        memory_text=memory_text,
                        target_visual=cand_visual,
                        target_text=cand_text,
                        return_components=True,
                        training_mode=False             # æ˜¾å¼è´Ÿé‡‡æ ·ï¼šå¿…é¡» False
                    )

                    # æ¨èæŸå¤±
                    lb_loss = info.get('lb_loss', None) if isinstance(info, dict) else None
                    rec_loss, _ = self.model.compute_loss(logits, labels, lb_loss=None)

                    # â€œæ¨¡æ€å¯¹é½â€æŸå¤±ï¼ˆStage 2/3ï¼‰ï¼šé»˜è®¤ç”¨ contrastive_lambda ä½œä¸ºæƒé‡
                    # å–æ¨¡å‹è¿”å›çš„ fused_repr / auxiliary_repr / seq_out (ä¼˜å…ˆ fused_repr)
                    align_loss = torch.tensor(0.0, device=self.device)
                    if self.contrastive_lambda > 0.0 and isinstance(info, dict):
                        rep = info.get('fused_repr', None)
                        if rep is None:
                            rep = info.get('auxiliary_repr', None)
                        if rep is None:
                            rep = info.get('seq_out', None)

                        if rep is not None:
                            # rep: [B, 1+N, D] -> æ­£æ ·æœ¬ä¸ºç¬¬0åˆ—
                            pos_rep = rep[:, 0, :] if rep.dim() == 3 else rep  # [B, D]

                            # å†»ç»“çš„ ID embedding ä½œä¸ºé”šç‚¹
                            id_emb = self._get_item_id_emb_batch(target_items)  # [B, D] æˆ– None
                            if id_emb is not None:
                                pos_rep_n = torch.nn.functional.normalize(pos_rep, dim=-1)
                                id_emb_n = torch.nn.functional.normalize(id_emb, dim=-1)
                                cos = (pos_rep_n * id_emb_n).sum(dim=-1)  # [B]
                                # surprise åŠ æƒï¼ˆå›°éš¾æ ·æœ¬æ›´å¼ºè°ƒå¯¹é½ï¼‰
                                # rec_loss æ˜¯ batch meanï¼›è¿™é‡Œç”¨ per-sample çš„ CE loss ä½œä¸º surprise çš„è¿‘ä¼¼
                                with torch.no_grad():
                                    per_sample_ce = -torch.log_softmax(logits.detach(), dim=1)[:, 0]
                                    surprise = torch.sigmoid(per_sample_ce)  # [B] in (0,1)
                                weights = 1.0 + 0.5 * surprise
                                align_loss = ((1.0 - cos) * weights).mean()

                    lb = lb_loss if lb_loss is not None else torch.tensor(0.0, device=self.device)
                    loss = rec_loss + self.contrastive_lambda * align_loss + 0.01 * lb

                # 5) åå‘ä¼ æ’­
                if self.scaler is not None:
                    self.scaler.scale(loss).backward()
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    loss.backward()
                    self.optimizer.step()

                # 6) Two-tier Memory Updateï¼ˆST always, LT when novelty is highï¼‰
                with torch.no_grad():
                    per_sample_loss = -torch.log_softmax(logits, dim=1)[:, 0]  # [B]
                    for i in range(bsz):
                        item_id = int(target_items[i].item())
                        loss_val = float(per_sample_loss[i].item())
                        # [Memory Update] æ–°ç‰ˆæœ¬å‚æ•°é¡ºåº: (item_id, id_emb, visual_emb, text_emb, loss_val)
                        self.local_memory.update(
                            item_id=item_id,
                            id_emb=self._get_item_id_emb(item_id),
                            visual_emb=self._get_item_img_emb(item_id),  # å‚æ•°åä» img_emb æ”¹ä¸º visual_emb
                            text_emb=self._get_item_text_emb(item_id),
                            loss_val=loss_val
                        )

                epoch_rec_loss += float(rec_loss.item())
                epoch_align_loss += float(align_loss.item())
                num_batches += 1

            epoch_rec_loss /= max(1, len(train_loader))
            epoch_align_loss /= max(1, len(train_loader))
            total_rec_loss += epoch_rec_loss
            total_align_loss += epoch_align_loss

        avg_rec_loss = total_rec_loss / max(1, self.local_epochs)
        avg_align_loss = total_align_loss / max(1, self.local_epochs)
        avg_total_loss = avg_rec_loss + self.contrastive_lambda * avg_align_loss

        metrics = {
            'loss': avg_total_loss,
            'rec_loss': avg_rec_loss,
            # ä¿æŒåŸå­—æ®µåï¼Œé¿å… server ç«¯æ—¥å¿—/ç”»å›¾æ–­æ‰
            'contrastive_loss': avg_align_loss,
            'memory_size': len(self.local_memory),
            # [Two-tierå…¼å®¹] total_updates = ST updates + LT updates
            'memory_updates': self.local_memory.total_updates_st + self.local_memory.total_updates_lt
        }

        if verbose:
            print(
                f"Client {self.client_id} | Loss: {avg_total_loss:.4f} "
                f"(Rec: {avg_rec_loss:.4f}, Align: {avg_align_loss:.4f}) | "
                f"Memory: {len(self.local_memory)}/{self.local_memory.capacity}"
            )

        return metrics

    def _query_memory_batch(self, target_items: torch.Tensor) -> Optional[Dict]:
        """
        æ‰¹é‡æŸ¥è¯¢æœ¬åœ°è®°å¿†ï¼ˆæ—§æ¥å£ï¼Œå·²å¼ƒç”¨ï¼‰

        Args:
            target_items: [B] ç›®æ ‡item IDs

        Returns:
            è®°å¿†æ£€ç´¢ç»“æœï¼Œç”¨äºæ³¨å…¥æ¨¡å‹
        """
        # å·²å¼ƒç”¨ï¼šä½¿ç”¨_retrieve_multimodal_memory_batchä»£æ›¿
        return None

    def _retrieve_multimodal_memory_batch(
        self,
        batch_size: int,
        top_k: int = 20
    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]:
        """
        ã€FedDMMRä¸“ç”¨ã€‘ä»æœ¬åœ°è®°å¿†ä¸­æ‰¹é‡æ£€ç´¢å¤šæ¨¡æ€ç‰¹å¾ï¼ˆTwo-tier: ST + LTï¼‰

        Args:
            batch_size: æ‰¹å¤§å°
            top_k: è¿”å›Top-Kä¸ªè®°å¿†ï¼ˆé»˜è®¤ä»STå’ŒLTæ··åˆæ£€ç´¢ï¼‰

        Returns:
            memory_visual: [B, TopK, img_dim] æˆ– None
            memory_text: [B, TopK, text_dim] æˆ– None

        Note:
            æ–°ç‰ˆæœ¬memoryè¿”å›4ä¸ªå€¼ (mem_vis, mem_txt, mem_id, mask)ï¼Œ
            æ­¤wrapperæ–¹æ³•åªè¿”å›å‰2ä¸ªä»¥ä¿æŒå‘åå…¼å®¹æ€§ã€‚
        """
        # [Memory Retrieval] æ–°ç‰ˆæœ¬è¿”å›4ä¸ªå€¼ï¼š(mem_vis, mem_txt, mem_id, mask)
        mem_vis, mem_txt, mem_id, mask = self.local_memory.retrieve_multimodal_memory_batch(
            batch_size=batch_size,
            top_k=top_k
        )

        # å‘åå…¼å®¹ï¼šåªè¿”å›visualå’Œtextï¼ˆå¿½ç•¥mem_idå’Œmaskï¼‰
        # å¦‚æœéœ€è¦maskæˆ–id_embï¼Œå¯ä»¥æ‰©å±•æ­¤æ¥å£
        return mem_vis, mem_txt

    def _get_candidate_visual_features(
        self,
        candidate_items: torch.Tensor
    ) -> Optional[torch.Tensor]:
        """
        [FIX 3] è·å–å€™é€‰ç‰©å“çš„è§†è§‰ç‰¹å¾ï¼ˆä»é¢„åŠ è½½çš„ç‰¹å¾çŸ©é˜µä¸­ç´¢å¼•ï¼‰

        æ¢¯åº¦æµéªŒè¯:
        - ä½¿ç”¨PyTorché«˜çº§ç´¢å¼•: visual_feats = self.item_visual_feats[valid_items]
        - æ­¤æ“ä½œæ”¯æŒåå‘ä¼ æ’­ï¼Œæ¢¯åº¦å¯ä»¥æµå‘item_visual_feats
        - æ— éœ€ä½¿ç”¨F.embeddingï¼Œç›´æ¥ç´¢å¼•å³å¯

        Args:
            candidate_items: [B, N] å€™é€‰ç‰©å“IDs

        Returns:
            visual_feats: [B, N, img_dim] æˆ– Noneï¼ˆå¦‚æœæœªåŠ è½½è§†è§‰ç‰¹å¾ï¼‰
        """
        if self.item_visual_feats is None:
            return None

        batch_size, num_candidates = candidate_items.shape

        # Clampåˆ°æœ‰æ•ˆèŒƒå›´ï¼Œé¿å…è¶Šç•Œ
        valid_items = torch.clamp(
            candidate_items,
            0,
            self.item_visual_feats.shape[0] - 1
        )

        # [FIX 3] ç´¢å¼•è§†è§‰ç‰¹å¾ [B, N, img_dim]
        # éªŒè¯: æ­¤æ“ä½œæ¢¯åº¦æµå®Œæ•´ï¼Œæ— éœ€ä¿®æ”¹
        visual_feats = self.item_visual_feats[valid_items]

        return visual_feats

    def _get_candidate_text_features(
        self,
        candidate_items: torch.Tensor
    ) -> Optional[torch.Tensor]:
        """
        [FIX 3] è·å–å€™é€‰ç‰©å“çš„æ–‡æœ¬ç‰¹å¾ï¼ˆä»é¢„åŠ è½½çš„ç‰¹å¾çŸ©é˜µä¸­ç´¢å¼•ï¼‰

        æ¢¯åº¦æµéªŒè¯:
        - ä½¿ç”¨PyTorché«˜çº§ç´¢å¼•: text_feats = self.item_text_feats[valid_items]
        - æ­¤æ“ä½œæ”¯æŒåå‘ä¼ æ’­ï¼Œæ¢¯åº¦å¯ä»¥æµå‘item_text_feats
        - æ— éœ€ä½¿ç”¨F.embeddingï¼Œç›´æ¥ç´¢å¼•å³å¯

        Args:
            candidate_items: [B, N] å€™é€‰ç‰©å“IDs

        Returns:
            text_feats: [B, N, text_dim] æˆ– Noneï¼ˆå¦‚æœæœªåŠ è½½æ–‡æœ¬ç‰¹å¾ï¼‰
        """
        if self.item_text_feats is None:
            return None

        batch_size, num_candidates = candidate_items.shape

        # Clampåˆ°æœ‰æ•ˆèŒƒå›´ï¼Œé¿å…è¶Šç•Œ
        valid_items = torch.clamp(
            candidate_items,
            0,
            self.item_text_feats.shape[0] - 1
        )

        # [FIX 3] ç´¢å¼•æ–‡æœ¬ç‰¹å¾ [B, N, text_dim]
        # éªŒè¯: æ­¤æ“ä½œæ¢¯åº¦æµå®Œæ•´ï¼Œæ— éœ€ä¿®æ”¹
        text_feats = self.item_text_feats[valid_items]

        return text_feats

    def _compute_contrastive_loss(
        self,
        user_ids: List[int],
        target_items: torch.Tensor
    ) -> torch.Tensor:
        """
        è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±

        ç›®æ ‡ï¼šå¯¹é½User Preference (Text) ä¸ Positive Item (Image/ID)

        Args:
            user_ids: ç”¨æˆ·IDs
            target_items: [B] ç›®æ ‡item IDs

        Returns:
            contrastive_loss: æ ‡é‡æŸå¤±
        """
        # ä½¿ç”¨æ¨¡å‹çš„compute_contrastive_lossæ–¹æ³•
        if hasattr(self.model, 'compute_contrastive_loss'):
            return self.model.compute_contrastive_loss(
                user_ids=user_ids,
                positive_items=target_items,
                negative_items=None,  # ä½¿ç”¨batchå†…è´Ÿæ ·æœ¬
                temperature=0.1
            )
        else:
            # å›é€€ï¼šè¿”å›0æŸå¤±
            return torch.tensor(0.0, device=self.device)

    def _get_item_id_emb_batch(self, item_ids: torch.Tensor) -> Optional[torch.Tensor]:
        """
        æ‰¹é‡è·å–ç‰©å“çš„ ID embeddingï¼ˆç”¨äº Stage 2/3 çš„å¯¹é½æŸå¤±ï¼‰

        Args:
            item_ids: [B] æˆ– [B, 1] çš„ item ids

        Returns:
            [B, D] æˆ– None
        """
        self._ensure_model_initialized()
        if item_ids is None:
            return None
        if item_ids.dim() > 1:
            item_ids = item_ids.view(-1)
        item_ids = item_ids.to(self.device)

        # ä¼˜å…ˆä½¿ç”¨æ¨¡å‹æä¾›çš„æ¥å£
        if hasattr(self.model, 'get_item_embeddings'):
            with torch.no_grad():
                emb = self.model.get_item_embeddings(item_ids, embedding_type='id')
            if emb is not None:
                if emb.dim() == 3:
                    emb = emb.squeeze(1)
                return emb

        # å›é€€ï¼šè®¿é—® SASRec å†…éƒ¨ embedding
        try:
            if hasattr(self.model, 'sasrec') and hasattr(self.model.sasrec, 'item_embedding'):
                with torch.no_grad():
                    return self.model.sasrec.item_embedding(item_ids)
        except Exception:
            return None
        return None

    def _get_item_text_emb(self, item_id: int) -> Optional[torch.Tensor]:
        """
        è·å–ç‰©å“çš„æ–‡æœ¬åµŒå…¥

        Args:
            item_id: ç‰©å“ID

        Returns:
            text_emb: æ–‡æœ¬åµŒå…¥ [emb_dim]
        """
        # ç›´æ¥ä»å­˜å‚¨çš„æ–‡æœ¬ç‰¹å¾ä¸­è·å–
        if self.item_text_feats is not None and item_id < self.item_text_feats.shape[0]:
            return self.item_text_feats[item_id].clone()
        return None

    def _get_item_img_emb(self, item_id: int) -> Optional[torch.Tensor]:
        """
        è·å–ç‰©å“çš„å›¾åƒåµŒå…¥

        Args:
            item_id: ç‰©å“ID

        Returns:
            img_emb: å›¾åƒåµŒå…¥ [emb_dim]
        """
        # ç›´æ¥ä»å­˜å‚¨çš„è§†è§‰ç‰¹å¾ä¸­è·å–
        if self.item_visual_feats is not None and item_id < self.item_visual_feats.shape[0]:
            return self.item_visual_feats[item_id].clone()
        return None

    def _get_item_id_emb(self, item_id: int) -> Optional[torch.Tensor]:
        """
        è·å–ç‰©å“çš„IDåµŒå…¥

        Args:
            item_id: ç‰©å“ID

        Returns:
            id_emb: IDåµŒå…¥ [emb_dim]
        """
        if self.model is not None and hasattr(self.model, 'get_item_embeddings'):
            item_tensor = torch.tensor([item_id], device=self.device)
            with torch.no_grad():
                emb = self.model.get_item_embeddings(item_tensor, embedding_type='id')
                if emb is not None:
                    return emb.squeeze(0)
        return None

    def _negative_sampling(
        self,
        batch_size: int,
        target_items: torch.Tensor
    ) -> torch.Tensor:
        """
        [Critical Fix] è´Ÿé‡‡æ ·ï¼šæ’é™¤ç”¨æˆ·å†å²äº¤äº’çš„æ‰€æœ‰ç‰©å“

        åœ¨è”é‚¦å•ç”¨æˆ·å®¢æˆ·ç«¯åœºæ™¯ä¸‹ï¼Œå¿…é¡»æ’é™¤ç”¨æˆ·çš„å®Œæ•´å†å²äº¤äº’ï¼Œè€Œä¸ä»…ä»…æ˜¯target_itemã€‚
        å¦åˆ™ä¼šäº§ç”Ÿ"ä¼ªè´Ÿæ ·æœ¬"ï¼šç”¨æˆ·äº¤äº’è¿‡çš„ç‰©å“è¢«å½“ä½œè´Ÿæ ·æœ¬ï¼Œç ´åè®­ç»ƒä¿¡å·ã€‚

        Args:
            batch_size: æ‰¹å¤§å°
            target_items: [B] æ­£æ ·æœ¬item IDs

        Returns:
            neg_items: [B, num_negatives] ä¿è¯ä¸åœ¨ç”¨æˆ·å†å²ä¸­çš„è´Ÿæ ·æœ¬
        """
        # [Critical Fix] ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰è´Ÿæ ·æœ¬ï¼ˆè¿‡é‡‡æ ·10å€ä»¥ç¡®ä¿è¶³å¤Ÿï¼‰
        # å› ä¸ºéœ€è¦æ’é™¤ç”¨æˆ·å†å²ï¼Œå¯èƒ½éœ€è¦å¤šæ¬¡é‡‡æ ·
        all_candidates = torch.randint(
            1, self.num_items,
            (batch_size, self.num_negatives * 10),  # 10å€è¿‡é‡‡æ ·
            device=self.device
        )  # [B, num_negatives*10]

        # [Critical Fix] åˆ›å»ºç”¨æˆ·å†å²ç‰©å“çš„mask
        # å¯¹äºè”é‚¦å­¦ä¹ ï¼Œbatchå†…æ‰€æœ‰æ ·æœ¬éƒ½æ¥è‡ªåŒä¸€ç”¨æˆ·ï¼Œä½¿ç”¨ç›¸åŒçš„user_items
        user_items_tensor = torch.tensor(list(self.user_items), device=self.device)  # [|history|]

        # å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œé€‰æ‹©ä¸åœ¨ç”¨æˆ·å†å²ä¸­çš„è´Ÿæ ·æœ¬
        neg_items = []
        for i in range(batch_size):
            candidates = all_candidates[i]  # [num_negatives*10]

            # [Critical Fix] æ’é™¤ç”¨æˆ·å†å²ï¼šä½¿ç”¨set membership check
            # æ–¹æ³•ï¼šå°†å€™é€‰è½¬ä¸ºCPU numpyï¼Œå¿«é€Ÿè¿‡æ»¤ï¼Œå†è½¬å›GPU
            candidates_np = candidates.cpu().numpy()
            valid_mask = np.array([item not in self.user_items for item in candidates_np])
            valid_negs = candidates[torch.from_numpy(valid_mask)]

            if len(valid_negs) >= self.num_negatives:
                # æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆè´Ÿæ ·æœ¬
                neg_items.append(valid_negs[:self.num_negatives])
            else:
                # [æå°‘æƒ…å†µ] ä¸å¤Ÿï¼Œç»§ç»­é‡‡æ ·ç›´åˆ°è¶³å¤Ÿ
                # è¿™ç§æƒ…å†µåœ¨ç”¨æˆ·å†å²å¾ˆé•¿æ—¶å¯èƒ½å‘ç”Ÿ
                collected = valid_negs.tolist()
                while len(collected) < self.num_negatives:
                    # é‡‡æ ·å•ä¸ªå€™é€‰å¹¶æ£€æŸ¥
                    candidate = torch.randint(1, self.num_items, (1,), device=self.device).item()
                    if candidate not in self.user_items:
                        collected.append(candidate)

                neg_items.append(torch.tensor(collected[:self.num_negatives], device=self.device))

        return torch.stack(neg_items)  # [B, num_negatives]

    def evaluate(
        self,
        user_sequences: Optional[Dict[int, List[int]]] = None,
        split: str = "test",
        k_list: List[int] = [5, 10, 20]
    ) -> Dict[str, float]:
        """
        è¯„ä¼°æ¨¡å‹

        Args:
            user_sequences: å®Œæ•´ç”¨æˆ·åºåˆ—å­—å…¸ï¼ˆç”¨äºè´Ÿé‡‡æ ·ï¼‰{user_id: [items]}
            split: 'val' æˆ– 'test'
            k_list: Top-Kåˆ—è¡¨

        Returns:
            metrics: è¯„ä¼°æŒ‡æ ‡
        """
        # æ ¹æ®é…ç½®é€‰æ‹©è¯„ä¼°æ–¹å¼
        if self.use_negative_sampling and user_sequences is not None:
            return self.evaluate_negative_sampling(user_sequences, split, k_list)

        # é»˜è®¤ä½¿ç”¨å…¨æ’åºè¯„ä¼°
        self._ensure_model_initialized()
        self.model.eval()

        dataset = self.val_dataset if split == "val" else self.test_dataset

        with torch.no_grad():
            batch = dataset[0]

            user_id = batch['user_id'].item()
            item_seq = batch['item_seq'].unsqueeze(0).to(self.device)
            target_item = batch['target_item'].item()

            # è®¡ç®—æ‰€æœ‰itemsçš„å¾—åˆ†
            # num_items = max_item_id + 1, so arange(1, num_items) = [1, ..., max_item_id]
            all_item_ids = torch.arange(1, self.num_items, device=self.device)
            all_item_ids_batch = all_item_ids.unsqueeze(0)  # [1, num_items-1]

            # ã€NEWã€‘ä»æœ¬åœ°è®°å¿†æ£€ç´¢å¤šæ¨¡æ€ç‰¹å¾ï¼ˆç”¨äºFedDMMRï¼‰
            memory_visual, memory_text = self._retrieve_multimodal_memory_batch(
                batch_size=1,
                top_k=20
            )

            # ã€NEWã€‘è·å–å€™é€‰ç‰©å“çš„å¤šæ¨¡æ€ç‰¹å¾
            target_visual = self._get_candidate_visual_features(all_item_ids_batch)
            target_text = self._get_candidate_text_features(all_item_ids_batch)

            # ã€NEWã€‘FedDMMRå‰å‘
            final_scores = self.model(
                user_ids=[user_id],
                input_seq=item_seq,
                target_items=all_item_ids_batch,
                memory_visual=memory_visual,    # [1, 20, img_dim] æˆ– None
                memory_text=memory_text,        # [1, 20, text_dim] æˆ– None
                target_visual=target_visual,    # [1, num_items-1, img_dim] æˆ– None
                target_text=target_text,        # [1, num_items-1, text_dim] æˆ– None
                return_components=False  # è¯„ä¼°æ—¶ä¸éœ€è¦é¢å¤–ä¿¡æ¯
            )

            scores = final_scores  # [1, num_items-1]

            # è·å–Top-K
            _, top_k_indices = torch.topk(scores, max(k_list), dim=1)
            top_k_items = all_item_ids[top_k_indices].squeeze(0).cpu().numpy()

            # è®¡ç®—æŒ‡æ ‡
            metrics = {}
            for k in k_list:
                top_k = top_k_items[:k]

                # HR@K
                hr = 1.0 if target_item in top_k else 0.0
                metrics[f'HR@{k}'] = hr

                # NDCG@K
                if target_item in top_k:
                    idx = np.where(top_k == target_item)[0][0]
                    ndcg = 1.0 / np.log2(idx + 2)
                else:
                    ndcg = 0.0
                metrics[f'NDCG@{k}'] = ndcg

            # MRR
            if target_item in top_k_items:
                rank = np.where(top_k_items == target_item)[0][0] + 1
                mrr = 1.0 / rank
            else:
                mrr = 0.0
            metrics['MRR'] = mrr

        return metrics

    def evaluate_negative_sampling(
        self,
        user_sequences: Dict[int, List[int]],
        split: str = "test",
        k_list: List[int] = [5, 10, 20]
    ) -> Dict[str, float]:
        """
        ä½¿ç”¨1:100è´Ÿé‡‡æ ·è¯„ä¼°æ¨¡å‹ï¼ˆå¯¹é½NCF/åŸå§‹SASRecè®ºæ–‡çš„è¯„ä¼°åè®®ï¼‰

        å¯¹æ¯ä¸ªæµ‹è¯•ç”¨æˆ·:
        1. è·å–Ground Truthç‰©å“
        2. éšæœºé‡‡æ ·Nä¸ªè´Ÿæ ·æœ¬ç‰©å“ï¼ˆä¸åœ¨ç”¨æˆ·å†å²äº¤äº’ä¸­ï¼‰
        3. æ„å»ºN+1ä¸ªå€™é€‰ç‰©å“é›†åˆ: [Ground Truth, Neg_1, ..., Neg_N]
        4. è®¡ç®—Ground Truthåœ¨è¿™N+1ä¸ªç‰©å“ä¸­çš„æ’å
        5. è®¡ç®—HR@Kå’ŒNDCG@KæŒ‡æ ‡

        Args:
            user_sequences: ç”¨æˆ·å®Œæ•´åºåˆ—å­—å…¸ {user_id: [items]}
            split: 'val' æˆ– 'test'
            k_list: è¯„ä¼°çš„Kå€¼åˆ—è¡¨

        Returns:
            metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        self._ensure_model_initialized()
        self.model.eval()

        dataset = self.val_dataset if split == "val" else self.test_dataset

        # ä¸ºå½“å‰ç”¨æˆ·å‡†å¤‡å€™é€‰è´Ÿæ ·æœ¬æ± 
        # ä»æ‰€æœ‰ç‰©å“ä¸­æ’é™¤ç”¨æˆ·å†å²äº¤äº’è¿‡çš„ç‰©å“
        user_id = self.client_id
        full_sequence = user_sequences[user_id]
        user_items = set(full_sequence)
        all_items = set(range(1, self.num_items))  # ç‰©å“IDèŒƒå›´: 1~num_items-1
        candidate_pool = list(all_items - user_items)

        # è¯„ä¼°æŒ‡æ ‡ç´¯åŠ å™¨
        all_hr = {k: [] for k in k_list}
        all_ndcg = {k: [] for k in k_list}

        with torch.no_grad():
            batch = dataset[0]

            user_id_val = batch['user_id'].item()
            item_seq = batch['item_seq'].unsqueeze(0).to(self.device)  # [1, seq_len]
            target_item = batch['target_item'].item()

            # ä»å€™é€‰æ± ä¸­éšæœºé‡‡æ ·Nä¸ªè´Ÿæ ·æœ¬
            if len(candidate_pool) < self.num_negatives_eval:
                negative_items = candidate_pool
            else:
                negative_items = np.random.choice(
                    candidate_pool,
                    size=self.num_negatives_eval,
                    replace=False
                ).tolist()

            # æ„å»ºN+1ä¸ªå€™é€‰ç‰©å“: [Ground Truth] + [Nä¸ªè´Ÿæ ·æœ¬]
            candidate_items = [target_item] + negative_items  # é•¿åº¦: N+1
            candidate_items_tensor = torch.tensor(
                candidate_items, dtype=torch.long
            ).unsqueeze(0).to(self.device)  # [1, N+1]

            # ã€NEWã€‘ä»æœ¬åœ°è®°å¿†æ£€ç´¢å¤šæ¨¡æ€ç‰¹å¾
            memory_visual, memory_text = self._retrieve_multimodal_memory_batch(
                batch_size=1,
                top_k=20
            )

            # ã€NEWã€‘è·å–å€™é€‰ç‰©å“çš„å¤šæ¨¡æ€ç‰¹å¾
            target_visual = self._get_candidate_visual_features(candidate_items_tensor)
            target_text = self._get_candidate_text_features(candidate_items_tensor)

            # ã€NEWã€‘FedMemå‰å‘ä¼ æ’­
            final_scores = self.model(
                user_ids=[user_id_val],
                input_seq=item_seq,
                target_items=candidate_items_tensor,
                memory_visual=memory_visual,
                memory_text=memory_text,
                target_visual=target_visual,
                target_text=target_text,
                return_components=False
            )

            scores = final_scores.squeeze()  # [N+1]

            # å¯¹å¾—åˆ†è¿›è¡Œæ’åºï¼Œè·å–æ’å
            # Ground Truthåœ¨ç´¢å¼•0ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°å®ƒçš„æ’å
            _, ranked_indices = torch.sort(scores, descending=True)
            ranked_indices = ranked_indices.cpu().numpy()

            # æ‰¾åˆ°Ground Truthï¼ˆç´¢å¼•0ï¼‰çš„æ’åä½ç½®
            rank = np.where(ranked_indices == 0)[0][0] + 1  # æ’åä»1å¼€å§‹

            # è®¡ç®—HR@Kå’ŒNDCG@K
            for k in k_list:
                # HR@K: Ground Truthæ˜¯å¦åœ¨Top-Kä¸­
                if rank <= k:
                    all_hr[k].append(1.0)
                    # NDCG@K: å¦‚æœåœ¨Top-Kä¸­ï¼Œè®¡ç®—NDCG
                    ndcg = 1.0 / np.log2(rank + 1)  # rankä»1å¼€å§‹ï¼Œlog2(rank+1)
                    all_ndcg[k].append(ndcg)
                else:
                    all_hr[k].append(0.0)
                    all_ndcg[k].append(0.0)

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        metrics = {}
        for k in k_list:
            metrics[f'HR@{k}'] = np.mean(all_hr[k])
            metrics[f'NDCG@{k}'] = np.mean(all_ndcg[k])

        # æ·»åŠ MRR
        mrr = 1.0 / rank if rank > 0 else 0.0
        metrics['MRR'] = mrr

        return metrics

    def get_memory_statistics(self) -> Dict:
        """è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
        return self.local_memory.get_statistics()

    def __repr__(self):
        return f"FedMemClient(id={self.client_id}, data={self.num_train_samples}, " \
               f"memory={len(self.local_memory)}/{self.local_memory.capacity})"
