"""
ç¦»çº¿ LLM ç”Ÿæˆæ¨¡å—

ç”¨äºç¦»çº¿ç”Ÿæˆç”¨æˆ·åå¥½æè¿°å’Œç‰©å“æè¿°ï¼Œä¸å‚ä¸åœ¨çº¿æ¨ç†ã€‚
"""
import torch
import json
from typing import List, Dict, Optional
from pathlib import Path
from tqdm import tqdm
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LLMPreferenceGenerator:
    """
    ä½¿ç”¨ LLM ç¦»çº¿ç”Ÿæˆç”¨æˆ·åå¥½æè¿°å’Œç‰©å“æè¿°
    """

    def __init__(
        self,
        llm_backend: str = "openai",
        model_name: str = "qwen-flash",
        api_key: Optional[str] = "sk-de8b84b8aca743cfa6fb42ec2776280b",
        base_url: Optional[str] = "https://dashscope.aliyuncs.com/compatible-mode/v1",
        cache_dir: str = "data/llm_cache",
        enable_thinking: bool = False
    ):
        """
        Args:
            llm_backend: LLM åç«¯ ('openai', 'anthropic', 'local')
            model_name: æ¨¡å‹åç§°
            api_key: API å¯†é’¥
            base_url: è‡ªå®šä¹‰APIåœ°å€ï¼ˆç”¨äºOpenAIå…¼å®¹APIï¼Œå¦‚vLLM, LocalAIç­‰ï¼‰
            cache_dir: ç¼“å­˜ç›®å½•
            enable_thinking: æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼ï¼ˆä»… DashScope qwen-flash ç­‰æ”¯æŒï¼‰
        """
        self.llm_backend = llm_backend
        self.model_name = model_name
        self.enable_thinking = enable_thinking
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        self._init_llm(api_key, base_url)

        # åŠ è½½ç¼“å­˜
        self.cache = self._load_cache()

    def _init_llm(self, api_key: Optional[str], base_url: Optional[str] = None):
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯

        Args:
            api_key: APIå¯†é’¥
            base_url: è‡ªå®šä¹‰APIåœ°å€ï¼ˆç”¨äºOpenAIå…¼å®¹APIï¼‰
        """
        if self.llm_backend == "openai":
            try:
                import openai
                if base_url:
                    self.client = openai.OpenAI(
                        api_key=api_key or "dummy-key",
                        base_url=base_url
                    )
                else:
                    self.client = openai.OpenAI(api_key=api_key)
            except ImportError:
                logger.error("è¯·å®‰è£… openai: pip install openai")
                self.client = None

        elif self.llm_backend == "anthropic":
            try:
                import anthropic
                self.client = anthropic.Anthropic(api_key=api_key)
            except ImportError:
                logger.error("è¯·å®‰è£… anthropic: pip install anthropic")
                self.client = None

        elif self.llm_backend == "local":
            try:
                from transformers import AutoModelForCausalLM, AutoTokenizer
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    device_map="auto",
                    torch_dtype=torch.float16
                )
            except ImportError:
                logger.error("è¯·å®‰è£… transformers: pip install transformers")
                self.client = None

    def _load_cache(self) -> Dict:
        """åŠ è½½ç¼“å­˜"""
        cache_file = self.cache_dir / "llm_cache.json"
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {"user_preferences": {}, "item_descriptions": {}}

    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        cache_file = self.cache_dir / "llm_cache.json"
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=2)

    def _call_llm(
        self,
        prompt: str,
        max_tokens: int = 500,
        extra_body: Optional[Dict] = None
    ) -> str:
        """è°ƒç”¨ LLM

        Args:
            prompt: æç¤ºè¯
            max_tokens: æœ€å¤§tokenæ•°
            extra_body: é¢å¤–å‚æ•°ï¼ˆå¦‚é˜¿é‡Œäº‘çš„{"enable_thinking": True}ï¼‰
        """
        if self.llm_backend == "openai" and self.client:
            request_params = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": 0.7
            }

            # æ·»åŠ æ€è€ƒæ¨¡å¼å‚æ•°
            if self.enable_thinking or extra_body:
                merged_extra = {}
                if self.enable_thinking:
                    merged_extra["enable_thinking"] = True
                if extra_body:
                    merged_extra.update(extra_body)
                request_params["extra_body"] = merged_extra

            response = self.client.chat.completions.create(**request_params)
            return response.choices[0].message.content

        elif self.llm_backend == "anthropic" and self.client:
            response = self.client.messages.create(
                model=self.model_name,
                max_tokens=max_tokens,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text

        elif self.llm_backend == "local":
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=0.7,
                do_sample=True
            )
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return response[len(prompt):].strip()

        else:
            return "LLM not available"

    def generate_user_preference(
        self,
        user_id: int,
        user_history: List[int],
        item_metadata: Dict[int, Dict]
    ) -> str:
        """
        ç”Ÿæˆç”¨æˆ·åå¥½æè¿°

        Args:
            user_id: ç”¨æˆ·ID
            user_history: ç”¨æˆ·å†å²äº¤äº’ç‰©å“IDåˆ—è¡¨
            item_metadata: ç‰©å“å…ƒæ•°æ®å­—å…¸

        Returns:
            ç”¨æˆ·åå¥½çš„æ–‡æœ¬æè¿°
        """
        cache_key = f"user_{user_id}"

        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.cache["user_preferences"]:
            return self.cache["user_preferences"][cache_key]

        # æ„å»ºæç¤º
        items_info = []
        for item_id in user_history[-20:]:  # æœ€è¿‘20ä¸ªç‰©å“
            if item_id in item_metadata:
                meta = item_metadata[item_id]
                items_info.append(f"- {meta.get('title', f'Item {item_id}')}")

        items_str = "\n".join(items_info)

        prompt = f"""Based on the user's historical interactions, summarize the user's preference characteristics.

User's historical interactions:
{items_str}

Please summarize the user's preferences in 2-3 sentences, including:
1. Preferred types/genres
2. Main features of interest
3. Potential interest directions

User preference summary:"""

        # è°ƒç”¨ LLMï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
        try:
            preference_text = self._call_llm(prompt, max_tokens=200)
        except Exception as e:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å†…å®¹å®¡æ ¸é”™è¯¯
            error_str = str(e).lower()
            if ("400" in error_str or "bad request" in error_str) and \
               ("data_inspection_failed" in error_str or "inappropriate content" in error_str):
                logger.warning(f"ç”¨æˆ· {user_id} è§¦å‘å†…å®¹å®¡æ ¸ï¼Œä½¿ç”¨å…œåº•æ–‡æœ¬")
                preference_text = "User has no obvious preferences."
            else:
                # å…¶ä»–é”™è¯¯ç»§ç»­æŠ›å‡º
                raise

        # ç¼“å­˜ç»“æœ
        self.cache["user_preferences"][cache_key] = preference_text
        self._save_cache()

        return preference_text

    def generate_item_description(
        self,
        item_id: int,
        item_metadata: Dict
    ) -> str:
        """
        ç”Ÿæˆç‰©å“æè¿°

        Args:
            item_id: ç‰©å“ID
            item_metadata: ç‰©å“å…ƒæ•°æ®

        Returns:
            ç‰©å“çš„æ–‡æœ¬æè¿°
        """
        cache_key = f"item_{item_id}"

        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.cache["item_descriptions"]:
            return self.cache["item_descriptions"][cache_key]

        # æ„å»ºæç¤º
        title = item_metadata.get('title', f'Item {item_id}')
        genres = item_metadata.get('genres', 'Unknown')

        prompt = f"""Generate a concise description for the following item for a recommendation system.

Item information:
- Title: {title}
- Genres: {genres}

Please describe the item's core features and target audience in 1-2 sentences.

Item description:"""

        # è°ƒç”¨ LLMï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
        try:
            description = self._call_llm(prompt, max_tokens=150)
        except Exception as e:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å†…å®¹å®¡æ ¸é”™è¯¯
            error_str = str(e).lower()
            if ("400" in error_str or "bad request" in error_str) and \
               ("data_inspection_failed" in error_str or "inappropriate content" in error_str):
                logger.warning(f"ç‰©å“ {item_id} ({title}) è§¦å‘å†…å®¹å®¡æ ¸ï¼Œä½¿ç”¨å…œåº•æ–‡æœ¬")
                description = "No description available."
            else:
                # å…¶ä»–é”™è¯¯ç»§ç»­æŠ›å‡º
                raise

        # ç¼“å­˜ç»“æœ
        self.cache["item_descriptions"][cache_key] = description
        self._save_cache()

        return description

    def batch_generate_user_preferences(
        self,
        users_data: List[Dict],
        item_metadata: Dict[int, Dict],
        save_path: str
    ):
        """
        æ‰¹é‡ç”Ÿæˆç”¨æˆ·åå¥½æè¿°

        Args:
            users_data: ç”¨æˆ·æ•°æ®åˆ—è¡¨ [{"user_id": ..., "user_history": [...]}, ...]
            item_metadata: ç‰©å“å…ƒæ•°æ®
            save_path: ä¿å­˜è·¯å¾„
        """
        logger.info(f"å¼€å§‹ç”Ÿæˆ {len(users_data)} ä¸ªç”¨æˆ·çš„åå¥½æè¿°...")

        preferences = {}

        for user_data in tqdm(users_data, desc="ç”Ÿæˆç”¨æˆ·åå¥½"):
            user_id = user_data['user_id']
            user_history = user_data['user_history']

            pref_text = self.generate_user_preference(
                user_id, user_history, item_metadata
            )

            preferences[user_id] = pref_text

        # ä¿å­˜
        save_file = Path(save_path)
        save_file.parent.mkdir(parents=True, exist_ok=True)

        with open(save_file, 'w', encoding='utf-8') as f:
            json.dump(preferences, f, ensure_ascii=False, indent=2)

        logger.info(f"ç”¨æˆ·åå¥½æè¿°å·²ä¿å­˜åˆ° {save_path}")

    def batch_generate_item_descriptions(
        self,
        item_metadata: Dict[int, Dict],
        save_path: str
    ):
        """
        æ‰¹é‡ç”Ÿæˆç‰©å“æè¿°

        Args:
            item_metadata: ç‰©å“å…ƒæ•°æ®å­—å…¸
            save_path: ä¿å­˜è·¯å¾„
        """
        logger.info(f"å¼€å§‹ç”Ÿæˆ {len(item_metadata)} ä¸ªç‰©å“çš„æè¿°...")

        descriptions = {}

        for item_id, meta in tqdm(item_metadata.items(), desc="ç”Ÿæˆç‰©å“æè¿°"):
            desc = self.generate_item_description(item_id, meta)
            descriptions[item_id] = desc

        # ä¿å­˜
        save_file = Path(save_path)
        save_file.parent.mkdir(parents=True, exist_ok=True)

        with open(save_file, 'w', encoding='utf-8') as f:
            json.dump(descriptions, f, ensure_ascii=False, indent=2)

        logger.info(f"ç‰©å“æè¿°å·²ä¿å­˜åˆ° {save_path}")


if __name__ == "__main__":
    """
    ç›´æ¥è¿è¡Œæ­¤è„šæœ¬ç”Ÿæˆ ML-100K æ•°æ®é›†çš„ç”¨æˆ·åå¥½å’Œç‰©å“æè¿°

    ä½¿ç”¨æ–¹æ³•:
        1. è®¾ç½® API å¯†é’¥:
           export DASHSCOPE_API_KEY="your-api-key"

        2. è¿è¡Œè„šæœ¬:
           python UR4Rec/models/llm_generator.py

        3. (å¯é€‰) è‡ªå®šä¹‰å‚æ•°:
           python UR4Rec/models/llm_generator.py --num_users 100 --num_items 500
    """
    import sys
    import os
    import argparse
    from pathlib import Path

    # æ·»åŠ é¡¹ç›®è·¯å¾„
    project_root = Path(__file__).parent.parent.parent
    sys.path.insert(0, str(project_root))

    from UR4Rec.data.dataset_loader import load_ml_100k

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='ç”Ÿæˆ ML-100K æ•°æ®é›†çš„ LLM åå¥½')
    parser.add_argument('--data_dir', type=str,
                       default='UR4Rec/data/Multimodal_Datasets',
                       help='æ•°æ®é›†ç›®å½•')
    parser.add_argument('--output_dir', type=str,
                       default='data/llm_generated',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--num_users', type=int, default=None,
                       help='ç”Ÿæˆçš„ç”¨æˆ·æ•°é‡ï¼ˆNone=å…¨éƒ¨ï¼‰')
    parser.add_argument('--num_items', type=int, default=None,
                       help='ç”Ÿæˆçš„ç‰©å“æ•°é‡ï¼ˆNone=å…¨éƒ¨ï¼‰')
    parser.add_argument('--model_name', type=str, default='qwen-flash',
                       help='LLM æ¨¡å‹åç§°')
    parser.add_argument('--enable_thinking', action='store_true',
                       help='å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼ï¼ˆä¼šæ¶ˆè€—æ›´å¤š tokensï¼‰')
    parser.add_argument('--skip_users', action='store_true',
                       help='è·³è¿‡ç”¨æˆ·åå¥½ç”Ÿæˆ')
    parser.add_argument('--skip_items', action='store_true',
                       help='è·³è¿‡ç‰©å“æè¿°ç”Ÿæˆ')

    args = parser.parse_args()

    print("=" * 60)
    print("ML-100K æ•°æ®é›† LLM åå¥½ç”Ÿæˆ")
    print("=" * 60)

    # æ£€æŸ¥ API å¯†é’¥
    api_key = os.getenv("DASHSCOPE_API_KEY") or os.getenv("OPENAI_API_KEY")

    if not api_key:
        print("\nâŒ é”™è¯¯: æœªè®¾ç½® API å¯†é’¥")
        print("\nè¯·è®¾ç½®ç¯å¢ƒå˜é‡:")
        print("  export DASHSCOPE_API_KEY='your-api-key'")
        print("  æˆ–")
        print("  export OPENAI_API_KEY='your-api-key'")
        print("\nè·å–é˜¿é‡Œäº‘ DashScope API å¯†é’¥:")
        print("  https://dashscope.aliyuncs.com/")
        sys.exit(1)

    # ç¡®å®šä½¿ç”¨å“ªä¸ªåç«¯
    if os.getenv("DASHSCOPE_API_KEY"):
        llm_backend = "openai"
        base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        print(f"\nä½¿ç”¨ DashScope API (æ¨¡å‹: {args.model_name})")
    else:
        llm_backend = "openai"
        base_url = None
        print(f"\nä½¿ç”¨ OpenAI API (æ¨¡å‹: {args.model_name})")

    # åŠ è½½æ•°æ®
    print(f"\n[1/4] åŠ è½½æ•°æ®é›†...")
    try:
        item_metadata, user_sequences, users = load_ml_100k(
            data_dir=args.data_dir,
            min_rating=4.0,
            min_seq_len=5
        )
        print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ")
        print(f"  - ç‰©å“æ•°: {len(item_metadata)}")
        print(f"  - ç”¨æˆ·åºåˆ—æ•°: {len(user_sequences)}")
    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print(f"\nè¯·ç¡®ä¿æ•°æ®é›†ç›®å½•å­˜åœ¨: {args.data_dir}/M_ML-100K/")
        sys.exit(1)

    # åˆ›å»º LLM ç”Ÿæˆå™¨
    print(f"\n[2/4] åˆ›å»º LLM ç”Ÿæˆå™¨...")
    generator = LLMPreferenceGenerator(
        llm_backend=llm_backend,
        model_name=args.model_name,
        api_key=api_key,
        base_url=base_url,
        cache_dir=f"{args.output_dir}/llm_cache",
        enable_thinking=args.enable_thinking
    )

    if args.enable_thinking:
        print("  âš ï¸  å·²å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼ï¼Œä¼šæ¶ˆè€—æ›´å¤š tokens")

    print(f"âœ“ ç”Ÿæˆå™¨åˆ›å»ºå®Œæˆ")

    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆç”¨æˆ·åå¥½
    if not args.skip_users:
        print(f"\n[3/4] ç”Ÿæˆç”¨æˆ·åå¥½...")

        # é€‰æ‹©ç”¨æˆ·å­é›†ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        users_to_generate = list(user_sequences.keys())
        if args.num_users:
            users_to_generate = users_to_generate[:args.num_users]
            print(f"  ç”Ÿæˆå‰ {args.num_users} ä¸ªç”¨æˆ·çš„åå¥½")

        # å‡†å¤‡ç”¨æˆ·æ•°æ®
        users_data = [
            {
                'user_id': user_id,
                'user_history': user_sequences[user_id]
            }
            for user_id in users_to_generate
        ]

        # æ‰¹é‡ç”Ÿæˆ
        user_pref_path = output_dir / "user_preferences.json"
        generator.batch_generate_user_preferences(
            users_data=users_data,
            item_metadata=item_metadata,
            save_path=str(user_pref_path)
        )

        print(f"âœ“ ç”¨æˆ·åå¥½å·²ä¿å­˜åˆ°: {user_pref_path}")
    else:
        print(f"\n[3/4] è·³è¿‡ç”¨æˆ·åå¥½ç”Ÿæˆ (--skip_users)")

    # ç”Ÿæˆç‰©å“æè¿°
    if not args.skip_items:
        print(f"\n[4/4] ç”Ÿæˆç‰©å“æè¿°...")

        # é€‰æ‹©ç‰©å“å­é›†ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        items_to_generate = dict(item_metadata)
        if args.num_items:
            items_to_generate = {
                k: v for k, v in list(item_metadata.items())[:args.num_items]
            }
            print(f"  ç”Ÿæˆå‰ {args.num_items} ä¸ªç‰©å“çš„æè¿°")

        # æ‰¹é‡ç”Ÿæˆ
        item_desc_path = output_dir / "item_descriptions.json"
        generator.batch_generate_item_descriptions(
            item_metadata=items_to_generate,
            save_path=str(item_desc_path)
        )

        print(f"âœ“ ç‰©å“æè¿°å·²ä¿å­˜åˆ°: {item_desc_path}")
    else:
        print(f"\n[4/4] è·³è¿‡ç‰©å“æè¿°ç”Ÿæˆ (--skip_items)")

    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    print(f"\nç¼“å­˜ä½ç½®: {generator.cache_dir}/llm_cache.json")
    print(f"  - ç”¨æˆ·åå¥½ç¼“å­˜: {len(generator.cache['user_preferences'])} æ¡")
    print(f"  - ç‰©å“æè¿°ç¼“å­˜: {len(generator.cache['item_descriptions'])} æ¡")

    print("\n" + "=" * 60)
    print("ç”Ÿæˆå®Œæˆï¼")
    print("=" * 60)

    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    if not args.skip_users:
        print(f"  âœ“ ç”¨æˆ·åå¥½: {output_dir}/user_preferences.json")
    if not args.skip_items:
        print(f"  âœ“ ç‰©å“æè¿°: {output_dir}/item_descriptions.json")
    print(f"  âœ“ ç¼“å­˜æ–‡ä»¶: {generator.cache_dir}/llm_cache.json")

    print("\nä¸‹ä¸€æ­¥:")
    print("  1. æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶å†…å®¹")
    print("  2. ä½¿ç”¨ç”Ÿæˆçš„åå¥½è¿›è¡Œè®­ç»ƒ:")
    print("     python UR4Rec/scripts/train.py --use_llm_features")

    print("\nğŸ’¡ æç¤º:")
    print("  - å¦‚æœ API è¯·æ±‚å¤±è´¥ï¼Œè„šæœ¬ä¼šä»ç¼“å­˜ä¸­æ¢å¤")
    print("  - é‡å¤è¿è¡Œè„šæœ¬ä¸ä¼šé‡å¤è°ƒç”¨ APIï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰")
    print("  - ä½¿ç”¨ --num_users å’Œ --num_items å¯ä»¥å…ˆç”Ÿæˆå°æ‰¹é‡æµ‹è¯•")
