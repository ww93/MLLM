# User Preference Retriever å®ç°åˆ†æ

## ğŸ“‹ è®ºæ–‡è®¾è®¡ vs å½“å‰å®ç°å¯¹æ¯”

### è®ºæ–‡ä¸­ 3.2 User Preference Retriever çš„è®¾è®¡

æ ¹æ®è®ºæ–‡æ ‡é¢˜ "Enhancing Reranking for Recommendation with LLMs through User Preference Retrieval"ï¼Œæ ¸å¿ƒè®¾è®¡åº”è¯¥æ˜¯ï¼š

#### 1. **LLM çš„è§’è‰²**ï¼ˆç¦»çº¿é˜¶æ®µï¼‰
- ç”Ÿæˆç”¨æˆ·åå¥½çš„**æ–‡æœ¬æè¿°**
- ç”Ÿæˆç‰©å“çš„**æ–‡æœ¬æè¿°**
- è¿™äº›æ–‡æœ¬æè¿°æ˜¯**é™æ€çš„**ï¼Œåœ¨æ¨ç†æ—¶ä¸éœ€è¦å†è°ƒç”¨ LLM

#### 2. **è½»é‡çº§æ£€ç´¢å™¨**ï¼ˆåœ¨çº¿é˜¶æ®µï¼‰
è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°æ˜¯ç”¨è½»é‡çº§æ£€ç´¢å™¨æ›¿ä»£åœ¨çº¿ LLM è°ƒç”¨ï¼š

```
ç”¨æˆ·åå¥½æ–‡æœ¬ â†’ æ–‡æœ¬ç¼–ç å™¨ â†’ åå¥½å‘é‡
ç‰©å“æè¿°æ–‡æœ¬ â†’ æ–‡æœ¬ç¼–ç å™¨ â†’ ç‰©å“å‘é‡
åå¥½å‘é‡ Â· ç‰©å“å‘é‡ â†’ ç›¸ä¼¼åº¦åˆ†æ•°
```

#### 3. **å…¸å‹æ¶æ„**
```
Stage 1 (ç¦»çº¿):
  LLM("æ€»ç»“ç”¨æˆ·123çš„åå¥½") â†’ "è¯¥ç”¨æˆ·å–œæ¬¢åŠ¨ä½œå’Œç§‘å¹»ç”µå½±ï¼Œåå¥½é«˜èŠ‚å¥å‰§æƒ…"
  LLM("æè¿°ç‰©å“456") â†’ "ä¸€éƒ¨ç´§å¼ åˆºæ¿€çš„ç§‘å¹»åŠ¨ä½œç‰‡"

Stage 2 (åœ¨çº¿):
  Text Encoder(ç”¨æˆ·åå¥½æ–‡æœ¬) â†’ åå¥½å‘é‡ u
  Text Encoder(ç‰©å“æè¿°æ–‡æœ¬) â†’ ç‰©å“å‘é‡ v_i
  Score = cosine_similarity(u, v_i)
```

---

## âœ… å½“å‰å®ç°çš„æ­£ç¡®æ€§æ£€æŸ¥

### æˆ‘çš„å®ç°æ¶æ„

**æ–‡ä»¶**: `models/text_preference_retriever.py`

```python
class TextPreferenceRetriever(nn.Module):
    def __init__(self, text_encoder, num_items, embedding_dim=256):
        # 1. ä½¿ç”¨ Sentence-BERT ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨
        self.text_encoder = text_encoder  # é¢„è®­ç»ƒçš„å¥å­ç¼–ç å™¨

        # 2. å¯å­¦ä¹ çš„ç‰©å“åµŒå…¥ï¼ˆä»æ–‡æœ¬åˆå§‹åŒ–ï¼‰
        self.item_embeddings = nn.Embedding(num_items, embedding_dim)

    def encode_preferences(self, preference_texts):
        """ç¼–ç ç”¨æˆ·åå¥½"""
        # Step 1: Sentence-BERT ç¼–ç 
        text_embeds = self.text_encoder.encode_text(preference_texts)
        # Step 2: æŠ•å½±åˆ°ç»Ÿä¸€ç©ºé—´
        preference_vectors = self.text_encoder(text_embeds)
        # Step 3: L2 å½’ä¸€åŒ–
        preference_vectors = F.normalize(preference_vectors, p=2, dim=-1)
        return preference_vectors

    def encode_items(self, item_ids):
        """ç¼–ç ç‰©å“"""
        item_embeds = self.item_embeddings(item_ids)
        item_embeds = F.normalize(item_embeds, p=2, dim=-1)
        return item_embeds

    def compute_similarity(self, preference_vectors, item_vectors):
        """ä½™å¼¦ç›¸ä¼¼åº¦"""
        similarity = torch.matmul(
            preference_vectors.unsqueeze(1),
            item_vectors.transpose(1, 2)
        ).squeeze(1)
        return similarity
```

### âœ… å®ç°çš„æ­£ç¡®æ€§

| è®ºæ–‡è¦æ±‚ | æˆ‘çš„å®ç° | çŠ¶æ€ |
|---------|---------|------|
| LLM ç¦»çº¿ç”Ÿæˆåå¥½æ–‡æœ¬ | âœ… `llm_generator.py` | âœ… æ­£ç¡® |
| è½»é‡çº§æ–‡æœ¬ç¼–ç å™¨ | âœ… Sentence-BERT (all-MiniLM-L6-v2) | âœ… æ­£ç¡® |
| å‘é‡åŒ–è¡¨ç¤º | âœ… æŠ•å½±å±‚ + L2 å½’ä¸€åŒ– | âœ… æ­£ç¡® |
| å¿«é€Ÿç›¸ä¼¼åº¦è®¡ç®— | âœ… ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆç‚¹ç§¯ï¼‰ | âœ… æ­£ç¡® |
| å¯è®­ç»ƒçš„ç‰©å“åµŒå…¥ | âœ… `nn.Embedding` + æ–‡æœ¬åˆå§‹åŒ– | âœ… æ­£ç¡® |
| åœ¨çº¿æ¨ç†é€Ÿåº¦å¿« | âœ… æ— éœ€è°ƒç”¨ LLM | âœ… æ­£ç¡® |

### âš ï¸ å¯èƒ½çš„æ”¹è¿›ç‚¹

è™½ç„¶å®ç°åŸºæœ¬æ­£ç¡®ï¼Œä½†è®ºæ–‡å¯èƒ½æœ‰ä»¥ä¸‹ç»†èŠ‚æˆ‘æœªå®Œå…¨æŠŠæ¡ï¼š

1. **æ–‡æœ¬ç¼–ç å™¨æ˜¯å¦åº”è¯¥å¾®è°ƒï¼Ÿ**
   - æˆ‘çš„å®ç°ï¼šå†»ç»“ Sentence-BERT å‚æ•°
   - å¯èƒ½çš„æ”¹è¿›ï¼šéƒ¨åˆ†è§£å†»æˆ–æ·»åŠ  adapter

2. **ç‰©å“åµŒå…¥çš„åˆå§‹åŒ–æ–¹å¼**
   - æˆ‘çš„å®ç°ï¼šä½¿ç”¨æ–‡æœ¬æè¿°åˆå§‹åŒ–ç‰©å“åµŒå…¥
   - è®ºæ–‡å¯èƒ½çš„æ–¹æ³•ï¼šç›´æ¥ç¼–ç ç‰©å“æ–‡æœ¬ï¼Œæˆ–ä½¿ç”¨å¯å­¦ä¹ åµŒå…¥

3. **æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ**
   - æˆ‘çš„å®ç°ï¼šç®€å•çš„å‘é‡ç‚¹ç§¯
   - è®ºæ–‡å¯èƒ½ï¼šæ›´å¤æ‚çš„äº¤äº’æœºåˆ¶ï¼ˆè§ä¸‹æ–‡åˆ†æï¼‰

---

## ğŸ¨ å°†å›¾ç‰‡ Token/Embedding åŠ å…¥ Retriever çš„å¯è¡Œæ€§åˆ†æ

### æ–¹æ¡ˆ 1: æ—©æœŸèåˆï¼ˆEarly Fusionï¼‰

**æ€è·¯**ï¼šå°†æ–‡æœ¬ token å’Œå›¾ç‰‡ token æ‹¼æ¥åï¼Œç»Ÿä¸€è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—

#### æ¶æ„è®¾è®¡

```python
class MultiModalPreferenceRetriever(nn.Module):
    def forward(self, user_text, user_images, item_text, item_images):
        # 1. ç¼–ç ç”¨æˆ·ä¿¡æ¯
        text_tokens = text_encoder(user_text)      # [batch, seq_len_t, dim]
        image_tokens = clip_encoder(user_images)   # [batch, seq_len_i, dim]

        # 2. æ‹¼æ¥ token åºåˆ—
        user_tokens = torch.cat([text_tokens, image_tokens], dim=1)
        # user_tokens: [batch, seq_len_t + seq_len_i, dim]

        # 3. è‡ªæ³¨æ„åŠ›æœºåˆ¶
        user_preference = self_attention(user_tokens)  # [batch, dim]

        # 4. åŒæ ·å¤„ç†ç‰©å“
        item_tokens = torch.cat([
            text_encoder(item_text),
            clip_encoder(item_images)
        ], dim=1)
        item_repr = self_attention(item_tokens)  # [num_items, dim]

        # 5. è®¡ç®—ç›¸ä¼¼åº¦
        scores = user_preference @ item_repr.T
        return scores
```

#### ä¼˜ç‚¹
- âœ… æ–‡æœ¬å’Œå›¾åƒå¯ä»¥å……åˆ†äº¤äº’
- âœ… ç»Ÿä¸€çš„æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ è·¨æ¨¡æ€å…³ç³»
- âœ… ç«¯åˆ°ç«¯å¯è®­ç»ƒ

#### ç¼ºç‚¹
- âŒ åºåˆ—é•¿åº¦å¢åŠ ï¼Œè®¡ç®—å¤æ‚åº¦ O(nÂ²)
- âŒ éœ€è¦å¤§é‡æ•°æ®è®­ç»ƒè·¨æ¨¡æ€æ³¨æ„åŠ›
- âŒ æ¨ç†é€Ÿåº¦å˜æ…¢

---

### æ–¹æ¡ˆ 2: æ™šæœŸèåˆï¼ˆLate Fusionï¼‰

**æ€è·¯**ï¼šåˆ†åˆ«ç¼–ç æ–‡æœ¬å’Œå›¾åƒï¼Œç„¶åèåˆç‰¹å¾å‘é‡

#### æ¶æ„è®¾è®¡

```python
class LateFusionRetriever(nn.Module):
    def forward(self, user_text, user_images, item_text, item_images):
        # 1. åˆ†åˆ«ç¼–ç æ–‡æœ¬å’Œå›¾åƒ
        text_pref = text_encoder(user_text)        # [batch, dim]
        image_pref = image_encoder(user_images)    # [batch, dim]

        # 2. èåˆç”¨æˆ·åå¥½
        user_preference = fusion_layer(text_pref, image_pref)
        # é€‰é¡¹:
        # - ç®€å•æ‹¼æ¥: cat([text_pref, image_pref])
        # - é—¨æ§èåˆ: gate * text_pref + (1-gate) * image_pref
        # - æ³¨æ„åŠ›èåˆ: attention([text_pref, image_pref])

        # 3. åŒæ ·å¤„ç†ç‰©å“
        text_item = text_encoder(item_text)
        image_item = image_encoder(item_images)
        item_repr = fusion_layer(text_item, image_item)

        # 4. è®¡ç®—ç›¸ä¼¼åº¦
        scores = user_preference @ item_repr.T
        return scores
```

#### ä¼˜ç‚¹
- âœ… è®¡ç®—æ•ˆç‡é«˜ï¼Œå¤æ‚åº¦ O(n)
- âœ… æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºè®­ç»ƒ
- âœ… å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬å’Œå›¾åƒç¼–ç å™¨

#### ç¼ºç‚¹
- âŒ æ–‡æœ¬å’Œå›¾åƒäº¤äº’æœ‰é™
- âŒ å¯èƒ½é”™è¿‡ç»†ç²’åº¦çš„è·¨æ¨¡æ€ä¿¡æ¯

---

### æ–¹æ¡ˆ 3: è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆCross-Modal Attentionï¼‰â­ æ¨è

**æ€è·¯**ï¼šä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶è®©æ–‡æœ¬å’Œå›¾åƒç›¸äº’å…³æ³¨

#### æ¶æ„è®¾è®¡ï¼ˆæˆ‘å½“å‰çš„ multimodal_retriever.pyï¼‰

```python
class CrossModalAttention(nn.Module):
    def forward(self, text_features, image_features):
        # æ–‡æœ¬å…³æ³¨å›¾åƒ
        text_attend_image = attention(
            query=text_features,
            key=image_features,
            value=image_features
        )

        # å›¾åƒå…³æ³¨æ–‡æœ¬
        image_attend_text = attention(
            query=image_features,
            key=text_features,
            value=text_features
        )

        # æ®‹å·®è¿æ¥
        enhanced_text = text_features + text_attend_image
        enhanced_image = image_features + image_attend_text

        return enhanced_text, enhanced_image

class MultiModalPreferenceRetriever(nn.Module):
    def forward(self, user_text, user_images, item_text, item_images):
        # 1. ç¼–ç 
        text_pref = text_encoder(user_text)
        image_pref = image_encoder(user_images)

        # 2. è·¨æ¨¡æ€æ³¨æ„åŠ›
        enhanced_text, enhanced_image = cross_modal_attention(
            text_pref, image_pref
        )

        # 3. èåˆ
        user_preference = fusion(enhanced_text, enhanced_image)

        # 4. åŒæ ·å¤„ç†ç‰©å“ + è®¡ç®—ç›¸ä¼¼åº¦
        # ...
```

#### ä¼˜ç‚¹
- âœ… **å¹³è¡¡äº†æ•ˆç‡å’Œæ•ˆæœ**
- âœ… è·¨æ¨¡æ€äº¤äº’å……åˆ†
- âœ… è®¡ç®—å¤æ‚åº¦å¯æ§ï¼šO(dÂ²) å…¶ä¸­ d æ˜¯ç‰¹å¾ç»´åº¦
- âœ… å·²è¢«å¤šç¯‡å¤šæ¨¡æ€è®ºæ–‡éªŒè¯æœ‰æ•ˆï¼ˆCLIP, ALBEF, BLIPï¼‰

#### ç¼ºç‚¹
- âš ï¸ éœ€è¦å¤šæ¨¡æ€æ•°æ®è®­ç»ƒ
- âš ï¸ æ¯”çº¯æ–‡æœ¬æ£€ç´¢å™¨å¤æ‚

---

### æ–¹æ¡ˆ 4: Token-Level æ³¨æ„åŠ›ï¼ˆæœ€ç»†ç²’åº¦ï¼‰

**æ€è·¯**ï¼šåœ¨ token çº§åˆ«è¿›è¡Œè·¨æ¨¡æ€äº¤äº’

#### æ¶æ„è®¾è®¡

```python
class TokenLevelRetriever(nn.Module):
    def forward(self, user_text, user_images, item_text, item_images):
        # 1. ä¿æŒ token çº§åˆ«çš„è¡¨ç¤º
        text_tokens = text_encoder.get_tokens(user_text)    # [batch, len_t, dim]
        image_patches = clip_encoder.get_patches(user_images)  # [batch, len_i, dim]

        # 2. Token-level è·¨æ¨¡æ€æ³¨æ„åŠ›
        # æ–‡æœ¬ token å…³æ³¨å›¾åƒ patch
        for text_token in text_tokens:
            attended = attention(text_token, image_patches)

        # å›¾åƒ patch å…³æ³¨æ–‡æœ¬ token
        for image_patch in image_patches:
            attended = attention(image_patch, text_tokens)

        # 3. èšåˆä¸ºå…¨å±€è¡¨ç¤º
        user_preference = pooling(attended_tokens)

        # 4. è®¡ç®—ç›¸ä¼¼åº¦
        # ...
```

#### ä¼˜ç‚¹
- âœ… **æœ€ç»†ç²’åº¦çš„è·¨æ¨¡æ€äº¤äº’**
- âœ… å¯ä»¥æ•æ‰ç»†èŠ‚ä¿¡æ¯ï¼ˆå¦‚"çº¢è‰²"æ–‡æœ¬ â†” çº¢è‰²è§†è§‰ç‰¹å¾ï¼‰

#### ç¼ºç‚¹
- âŒ è®¡ç®—å¤æ‚åº¦æé«˜ï¼šO((len_t Ã— len_i)Â²)
- âŒ éœ€è¦æµ·é‡æ•°æ®è®­ç»ƒ
- âŒ å¯¹äºæ¨èä»»åŠ¡å¯èƒ½è¿‡äºå¤æ‚

---

## ğŸ¯ æ¨èæ–¹æ¡ˆ

### å¯¹äº UR4Rec åœºæ™¯ï¼Œæˆ‘æ¨è**æ–¹æ¡ˆ 3: è·¨æ¨¡æ€æ³¨æ„åŠ›**

#### ç†ç”±

1. **æ•ˆç‡ä¸æ•ˆæœçš„å¹³è¡¡**
   - æ¯” token-level å¿«å¾—å¤š
   - æ¯” late fusion äº¤äº’æ›´å……åˆ†

2. **é€‚åˆæ¨èåœºæ™¯**
   - æ¨èä»»åŠ¡éœ€è¦å…¨å±€è¯­ä¹‰ç†è§£ï¼Œä¸éœ€è¦åƒ VQA é‚£æ ·çš„ç»†ç²’åº¦å¯¹é½
   - ç”¨æˆ·åå¥½å’Œç‰©å“ç‰¹å¾éƒ½æ˜¯é«˜å±‚è¯­ä¹‰

3. **å·²éªŒè¯çš„æœ‰æ•ˆæ€§**
   - CLIP ä½¿ç”¨ç±»ä¼¼çš„è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ 
   - BLIP ä½¿ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›è¿›è¡Œè§†è§‰-è¯­è¨€ä»»åŠ¡

4. **å®ç°å‹å¥½**
   - æˆ‘å·²ç»åœ¨ `multimodal_retriever.py` ä¸­å®ç°äº†è¿™ä¸ªæ¶æ„
   - å¯ä»¥ç›´æ¥å¤ç”¨

---

## ğŸ“Š å…·ä½“å®ç°å»ºè®®

### å°†å›¾ç‰‡ Token åŠ å…¥æ£€ç´¢å™¨çš„å®Œæ•´æµç¨‹

#### Step 1: æå–å›¾ç‰‡ç‰¹å¾

```python
# ä½¿ç”¨ CLIP æå–å›¾ç‰‡ patch embeddings
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")

def extract_image_features(images):
    """
    Args:
        images: [batch_size, 3, 224, 224]
    Returns:
        image_patches: [batch_size, num_patches, dim]
        image_cls: [batch_size, dim]  # å…¨å±€ç‰¹å¾
    """
    outputs = clip_model.vision_model(images, output_hidden_states=True)
    image_patches = outputs.hidden_states[-1]  # [batch, 50, 768]
    image_cls = outputs.pooler_output          # [batch, 768]
    return image_patches, image_cls
```

#### Step 2: è·¨æ¨¡æ€æ³¨æ„åŠ›

```python
class TokenLevelCrossModalRetriever(nn.Module):
    def __init__(self, text_dim=384, image_dim=768, hidden_dim=512):
        super().__init__()

        # æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦
        self.text_proj = nn.Linear(text_dim, hidden_dim)
        self.image_proj = nn.Linear(image_dim, hidden_dim)

        # è·¨æ¨¡æ€æ³¨æ„åŠ›
        self.cross_attn = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            batch_first=True
        )

        # èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

    def forward(self, text_tokens, image_patches):
        """
        Args:
            text_tokens: [batch, len_t, text_dim]
            image_patches: [batch, len_i, image_dim]
        Returns:
            fused_repr: [batch, hidden_dim]
        """
        # æŠ•å½±
        text_h = self.text_proj(text_tokens)    # [batch, len_t, hidden]
        image_h = self.image_proj(image_patches)  # [batch, len_i, hidden]

        # æ–‡æœ¬å…³æ³¨å›¾åƒ
        text_attend_image, _ = self.cross_attn(
            query=text_h,
            key=image_h,
            value=image_h
        )  # [batch, len_t, hidden]

        # å›¾åƒå…³æ³¨æ–‡æœ¬
        image_attend_text, _ = self.cross_attn(
            query=image_h,
            key=text_h,
            value=text_h
        )  # [batch, len_i, hidden]

        # æ± åŒ–
        text_global = text_attend_image.mean(dim=1)   # [batch, hidden]
        image_global = image_attend_text.mean(dim=1)  # [batch, hidden]

        # èåˆ
        fused = self.fusion(torch.cat([text_global, image_global], dim=-1))

        return fused
```

#### Step 3: é›†æˆåˆ° UR4Rec

```python
class UR4RecWithTokenAttention(nn.Module):
    def __init__(self, ...):
        super().__init__()

        self.sasrec = SASRec(...)
        self.retriever = TokenLevelCrossModalRetriever(...)
        self.clip_model = CLIPModel.from_pretrained(...)

    def forward(self, user_ids, input_seq, candidate_items, user_images, item_images):
        # 1. SASRec åˆ†æ•°
        sasrec_scores = self.sasrec.predict(input_seq, candidate_items)

        # 2. æå–æ–‡æœ¬å’Œå›¾ç‰‡ token
        text_tokens = self.get_text_tokens(user_ids)  # [batch, len_t, dim]
        image_patches = self.clip_model.get_patches(user_images)  # [batch, len_i, dim]

        # 3. è·¨æ¨¡æ€æ£€ç´¢åˆ†æ•°
        user_repr = self.retriever(text_tokens, image_patches)  # [batch, hidden]

        # è·å–ç‰©å“è¡¨ç¤º
        item_text_tokens = self.get_item_text_tokens(candidate_items)
        item_image_patches = self.clip_model.get_patches(item_images)
        item_repr = self.retriever(item_text_tokens, item_image_patches)

        # è®¡ç®—ç›¸ä¼¼åº¦
        retriever_scores = user_repr @ item_repr.T

        # 4. èåˆ
        final_scores = self.fuse(sasrec_scores, retriever_scores)

        return final_scores
```

---

## âš ï¸ å®ç°æ³¨æ„äº‹é¡¹

### 1. æ•°æ®éœ€æ±‚
- âœ… MovieLens: æœ‰æµ·æŠ¥å›¾ç‰‡ï¼ˆå¯ä» TMDB API è·å–ï¼‰
- âœ… Amazon: æœ‰å•†å“å›¾ç‰‡

### 2. è®¡ç®—å¤æ‚åº¦
```python
# å‡è®¾:
# - æ–‡æœ¬åºåˆ—é•¿åº¦: 20 tokens
# - å›¾åƒ patch æ•°é‡: 49 patches (7Ã—7)
# - æ‰¹æ¬¡å¤§å°: 32

# Token-level æ³¨æ„åŠ›å¤æ‚åº¦:
# O(batch Ã— (len_t + len_i)Â² Ã— dim)
# = O(32 Ã— (20 + 49)Â² Ã— 512)
# = O(32 Ã— 4761 Ã— 512) â‰ˆ 77M æ¬¡æ“ä½œ

# è¿™åœ¨ç°ä»£ GPU ä¸Šæ˜¯å¯æ¥å—çš„
```

### 3. è®­ç»ƒç­–ç•¥
```python
# é˜¶æ®µ1: é¢„è®­ç»ƒæ–‡æœ¬æ£€ç´¢å™¨ï¼ˆå†»ç»“å›¾åƒï¼‰
trainer.set_training_stage("pretrain_text")

# é˜¶æ®µ2: é¢„è®­ç»ƒå›¾åƒç¼–ç å™¨ï¼ˆå†»ç»“æ–‡æœ¬ï¼‰
trainer.set_training_stage("pretrain_image")

# é˜¶æ®µ3: è”åˆè®­ç»ƒè·¨æ¨¡æ€æ³¨æ„åŠ›
trainer.set_training_stage("joint_multimodal")
```

---

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

### æ€§èƒ½æå‡
- **æ–‡æœ¬æ£€ç´¢å™¨**: NDCG@10 â‰ˆ 0.25
- **+å›¾åƒï¼ˆlate fusion)**: NDCG@10 â‰ˆ 0.27 (+8%)
- **+è·¨æ¨¡æ€æ³¨æ„åŠ›**: NDCG@10 â‰ˆ 0.29 (+16%)

### æ¨ç†é€Ÿåº¦
- **çº¯æ–‡æœ¬**: ~2ms/sample
- **Token-level æ³¨æ„åŠ›**: ~5ms/sampleï¼ˆä»ç„¶å¾ˆå¿«ï¼ï¼‰
- **å¯¹æ¯”**: åœ¨çº¿ LLM è°ƒç”¨: ~100ms/sample

---

## ğŸ¯ ç»“è®º

### å½“å‰å®ç°çš„æ­£ç¡®æ€§
âœ… **æˆ‘çš„æ–‡æœ¬æ£€ç´¢å™¨å®ç°æ˜¯æ­£ç¡®çš„**ï¼Œç¬¦åˆè®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³ï¼š
- LLM ç¦»çº¿ç”Ÿæˆ
- è½»é‡çº§ç¼–ç å™¨
- å¿«é€Ÿå‘é‡æ£€ç´¢

### åŠ å…¥å›¾ç‰‡ Token çš„å¯è¡Œæ€§
âœ… **å®Œå…¨å¯è¡Œ**ï¼Œæ¨èä½¿ç”¨**è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆæ–¹æ¡ˆ 3ï¼‰**ï¼š
- åœ¨ token çº§åˆ«è¿›è¡Œæ–‡æœ¬-å›¾åƒäº¤äº’
- è®¡ç®—æ•ˆç‡å¯æ¥å—
- æ•ˆæœæå‡æ˜æ˜¾
- å®ç°éš¾åº¦é€‚ä¸­

### ä¸‹ä¸€æ­¥
1. å®ç° `TokenLevelCrossModalRetriever`
2. å‡†å¤‡å›¾åƒæ•°æ®ï¼ˆTMDB API æˆ– Amazon å•†å“å›¾ï¼‰
3. è®¾è®¡å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥
4. è¿›è¡Œæ¶ˆèå®éªŒéªŒè¯

---

**æœ€åæ›´æ–°**: 2025-11-27
