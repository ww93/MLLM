â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    UR4Rec Project Successfully Created               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Paper: "Enhancing Reranking for Recommendation with LLMs through 
        User Preference Retrieval" (COLING 2025)

Authors: Haobo Zhang, Qiannan Zhu, Zhicheng Dou

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š PROJECT STATISTICS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Total Files:          19
Python Code Lines:    ~2,400
Documentation:        3 comprehensive guides

ğŸ“ PROJECT STRUCTURE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

UR4Rec/
â”œâ”€â”€ ğŸ“– Documentation
â”‚   â”œâ”€â”€ README.md              (Main documentation - comprehensive guide)
â”‚   â”œâ”€â”€ QUICKSTART.md          (5-minute quick start guide)
â”‚   â””â”€â”€ PROJECT_SUMMARY.md     (Technical overview and summary)
â”‚
â”œâ”€â”€ ğŸ§  Models (Core Implementation)
â”‚   â”œâ”€â”€ user_preference_retriever.py   (Transformer-based retriever)
â”‚   â”œâ”€â”€ llm_reranker.py                (LLM integration layer)
â”‚   â””â”€â”€ ur4rec.py                      (Main UR4Rec framework)
â”‚
â”œâ”€â”€ ğŸ› ï¸ Utilities
â”‚   â”œâ”€â”€ data_loader.py         (Data processing & loading)
â”‚   â””â”€â”€ metrics.py             (Evaluation metrics: NDCG, HR, MRR, etc.)
â”‚
â”œâ”€â”€ ğŸš€ Scripts
â”‚   â”œâ”€â”€ train.py              (Training script with full features)
â”‚   â”œâ”€â”€ evaluate.py           (Evaluation with/without LLM)
â”‚   â””â”€â”€ demo.py               (Quick demo script)
â”‚
â”œâ”€â”€ âš™ï¸ Configuration
â”‚   â”œâ”€â”€ configs/default_config.yaml    (Default hyperparameters)
â”‚   â”œâ”€â”€ requirements.txt               (Dependencies)
â”‚   â””â”€â”€ setup.py                       (Package installation)
â”‚
â””â”€â”€ ğŸ““ Notebooks
    â””â”€â”€ quickstart.ipynb       (Interactive Jupyter tutorial)

âœ¨ KEY FEATURES IMPLEMENTED
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… User Preference Retriever
   â€¢ Multi-layer Transformer encoder (4 layers, 8 heads)
   â€¢ Positional encoding for sequential modeling
   â€¢ Cross-attention mechanism for candidate-history interaction
   â€¢ Configurable architecture (embedding dim, layers, heads)

âœ… LLM Integration
   â€¢ OpenAI API support (GPT-3.5, GPT-4)
   â€¢ Anthropic API support (Claude 3)
   â€¢ Local LLM support (HuggingFace models)
   â€¢ Customizable prompt templates
   â€¢ Batch inference support

âœ… Training Pipeline
   â€¢ Efficient data loading with automatic padding
   â€¢ Binary cross-entropy loss with masking
   â€¢ Adam optimizer + learning rate scheduling
   â€¢ Early stopping & model checkpointing
   â€¢ TensorBoard integration for monitoring

âœ… Evaluation System
   â€¢ NDCG@K (Normalized Discounted Cumulative Gain)
   â€¢ HR@K (Hit Rate)
   â€¢ MRR@K (Mean Reciprocal Rank)
   â€¢ Precision@K, Recall@K, MAP@K
   â€¢ Comparison: Retriever-only vs LLM-enhanced

âœ… Additional Features
   â€¢ Sample data generation for testing
   â€¢ Flexible data formats (JSON, CSV, PKL)
   â€¢ Comprehensive documentation
   â€¢ Interactive Jupyter notebook
   â€¢ Production-ready code structure

ğŸš€ QUICK START
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. Install dependencies:
   pip install -r requirements.txt

2. Run the demo:
   python demo.py
   
   Or: ./run_demo.sh

3. Train on your data:
   python scripts/train.py --config configs/default_config.yaml

4. Evaluate with LLM:
   export OPENAI_API_KEY="your-key"
   python scripts/evaluate.py --checkpoint outputs/best_model.pt --use_llm

5. Explore interactively:
   jupyter notebook notebooks/quickstart.ipynb

ğŸ“š USAGE EXAMPLES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Python API:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from models import UR4Rec
import torch

# Create model
model = UR4Rec(
    num_items=10000,
    embedding_dim=256,
    num_layers=4,
    use_llm=True,
    llm_backend="openai"
)

# Make predictions
user_history = torch.tensor([[101, 205, 303, 412]])
candidates = torch.tensor([[801, 802, 803, 804, 805]])

with torch.no_grad():
    rankings = model.predict(user_history, candidates)

print(f"Ranked items: {rankings[0]}")

Command Line:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Training
python scripts/train.py \
    --train_data data/train.json \
    --num_items 10000 \
    --batch_size 32 \
    --num_epochs 50

# Evaluation
python scripts/evaluate.py \
    --checkpoint outputs/best_model.pt \
    --use_llm \
    --llm_backend openai

ğŸ”§ CONFIGURATION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Default Hyperparameters:
â€¢ embedding_dim: 256
â€¢ num_layers: 4
â€¢ num_heads: 8
â€¢ d_ff: 1024
â€¢ dropout: 0.1
â€¢ batch_size: 32
â€¢ learning_rate: 0.001

Customize via:
â€¢ Command-line arguments
â€¢ configs/default_config.yaml
â€¢ Python API parameters

ğŸ“Š EXPECTED PERFORMANCE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Retriever-only:
â€¢ Fast inference (~1ms per sample on GPU)
â€¢ Good baseline performance

With LLM Reranking:
â€¢ Improved ranking quality (paper reports significant gains)
â€¢ Slower inference (~100ms per sample with API calls)
â€¢ Cost considerations for commercial LLM APIs

ğŸ“– DOCUMENTATION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. README.md          - Comprehensive documentation (50+ sections)
2. QUICKSTART.md      - Get started in 5 minutes
3. PROJECT_SUMMARY.md - Technical details and architecture
4. Inline code docs   - Detailed docstrings in all modules
5. Jupyter notebook   - Interactive tutorial with visualizations

ğŸ¯ NEXT STEPS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. Run the demo to verify installation:
   python demo.py

2. Read the QUICKSTART.md for basic usage

3. Explore the Jupyter notebook:
   jupyter notebook notebooks/quickstart.ipynb

4. Prepare your own dataset in the required format

5. Train on your data:
   python scripts/train.py --train_data your_data.json

6. Set up LLM API keys for enhanced reranking:
   export OPENAI_API_KEY="your-key"

7. Evaluate and compare results

ğŸ“„ CITATION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@inproceedings{zhang2025enhancing,
  title={Enhancing Reranking for Recommendation with LLMs through 
         User Preference Retrieval},
  author={Zhang, Haobo and Zhu, Qiannan and Dou, Zhicheng},
  booktitle={Proceedings of the 31st International Conference on 
             Computational Linguistics},
  pages={658--671},
  year={2025}
}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ¨ Project is ready to use! Start with: python demo.py

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
