# UR4Rec V2 è®­ç»ƒæŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»å¦‚ä½•è®­ç»ƒ UR4Rec V2 æ¨¡å‹ã€‚

## ğŸ“‹ ç›®å½•

1. [è®­ç»ƒæµç¨‹æ¦‚è§ˆ](#è®­ç»ƒæµç¨‹æ¦‚è§ˆ)
2. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
3. [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
4. [LLM æ•°æ®ç”Ÿæˆ](#llm-æ•°æ®ç”Ÿæˆ)
5. [æ¨¡å‹è®­ç»ƒ](#æ¨¡å‹è®­ç»ƒ)
6. [è®­ç»ƒé˜¶æ®µè¯¦è§£](#è®­ç»ƒé˜¶æ®µè¯¦è§£)
7. [é…ç½®æ–‡ä»¶è¯´æ˜](#é…ç½®æ–‡ä»¶è¯´æ˜)
8. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## è®­ç»ƒæµç¨‹æ¦‚è§ˆ

UR4Rec V2 çš„å®Œæ•´è®­ç»ƒæµç¨‹åˆ†ä¸ºä»¥ä¸‹æ­¥éª¤ï¼š

```
1. æ•°æ®é¢„å¤„ç†
   â†“
2. LLM ç¦»çº¿ç”Ÿæˆç”¨æˆ·åå¥½å’Œç‰©å“æè¿°
   â†“
3. å¤šé˜¶æ®µè®­ç»ƒ
   â”œâ”€ é˜¶æ®µ1: é¢„è®­ç»ƒ SASRec
   â”œâ”€ é˜¶æ®µ2: é¢„è®­ç»ƒæ£€ç´¢å™¨
   â”œâ”€ é˜¶æ®µ3: è”åˆå¾®è°ƒ
   â””â”€ é˜¶æ®µ4: ç«¯åˆ°ç«¯ä¼˜åŒ–
   â†“
4. æ¨¡å‹è¯„ä¼°å’Œä¿å­˜
```

---

## ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

ä¸»è¦ä¾èµ–åŒ…æ‹¬ï¼š
- PyTorch >= 2.0.0
- sentence-transformers
- transformers
- numpy, pandas
- pyyaml
- tqdm

### 2. ï¼ˆå¯é€‰ï¼‰é…ç½® LLM API

å¦‚æœä½¿ç”¨çœŸå®çš„ LLMï¼ˆé Mockï¼‰ï¼š

**OpenAI**:
```bash
export OPENAI_API_KEY="your-api-key"
```

**Anthropic**:
```bash
export ANTHROPIC_API_KEY="your-api-key"
```

---

## æ•°æ®å‡†å¤‡

### 1. ä¸‹è½½å’Œé¢„å¤„ç†æ•°æ®é›†

#### MovieLens-100K

```bash
python scripts/preprocess_movielens.py \
    --dataset ml-100k \
    --output_dir data/ml-100k \
    --num_candidates 100
```

#### MovieLens-1M

```bash
python scripts/preprocess_movielens.py \
    --dataset ml-1m \
    --output_dir data/ml-1m \
    --num_candidates 100
```

#### Amazon Beauty

```bash
python scripts/preprocess_beauty.py \
    --input_file data/raw/beauty.json \
    --output_dir data/beauty \
    --num_candidates 100
```

### 2. éªŒè¯æ•°æ®

é¢„å¤„ç†å®Œæˆåï¼Œæ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š

```
data/ml-100k/
â”œâ”€â”€ train_sequences.npy      # è®­ç»ƒåºåˆ—
â”œâ”€â”€ val_sequences.npy        # éªŒè¯åºåˆ—
â”œâ”€â”€ test_sequences.npy       # æµ‹è¯•åºåˆ—
â”œâ”€â”€ item_metadata.json       # ç‰©å“å…ƒæ•°æ®
â”œâ”€â”€ item_map.json           # ç‰©å“IDæ˜ å°„
â””â”€â”€ user_map.json           # ç”¨æˆ·IDæ˜ å°„
```

---

## LLM æ•°æ®ç”Ÿæˆ

### 1. ä½¿ç”¨ Mock ç”Ÿæˆå™¨ï¼ˆæ— éœ€ APIï¼‰

æ¨èç”¨äºå¿«é€Ÿæµ‹è¯•ï¼š

```bash
python scripts/generate_llm_data.py \
    --config configs/movielens_100k.yaml \
    --data_dir data/ml-100k \
    --output_dir data/ml-100k/llm_generated \
    --llm_backend mock
```

### 2. ä½¿ç”¨ OpenAI

```bash
python scripts/generate_llm_data.py \
    --config configs/movielens_100k.yaml \
    --data_dir data/ml-100k \
    --output_dir data/ml-100k/llm_generated \
    --llm_backend openai \
    --model_name gpt-3.5-turbo \
    --api_key $OPENAI_API_KEY
```

### 3. ä½¿ç”¨ Anthropic

```bash
python scripts/generate_llm_data.py \
    --config configs/movielens_100k.yaml \
    --data_dir data/ml-100k \
    --output_dir data/ml-100k/llm_generated \
    --llm_backend anthropic \
    --model_name claude-3-haiku-20240307 \
    --api_key $ANTHROPIC_API_KEY
```

### 4. ç”Ÿæˆé€‰é¡¹

```bash
# é™åˆ¶ç”Ÿæˆæ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
python scripts/generate_llm_data.py \
    --config configs/movielens_100k.yaml \
    --data_dir data/ml-100k \
    --output_dir data/ml-100k/llm_generated \
    --llm_backend mock \
    --max_users 100 \
    --max_items 500

# åªç”Ÿæˆç”¨æˆ·åå¥½
python scripts/generate_llm_data.py \
    --config configs/movielens_100k.yaml \
    --data_dir data/ml-100k \
    --output_dir data/ml-100k/llm_generated \
    --llm_backend mock \
    --skip_items

# åªç”Ÿæˆç‰©å“æè¿°
python scripts/generate_llm_data.py \
    --config configs/movielens_100k.yaml \
    --data_dir data/ml-100k \
    --output_dir data/ml-100k/llm_generated \
    --llm_backend mock \
    --skip_users
```

### 5. éªŒè¯ç”Ÿæˆç»“æœ

æ£€æŸ¥è¾“å‡ºç›®å½•ï¼š

```
data/ml-100k/llm_generated/
â”œâ”€â”€ user_preferences.json    # ç”¨æˆ·åå¥½æè¿°
â””â”€â”€ item_descriptions.json   # ç‰©å“æ–‡æœ¬æè¿°
```

æŸ¥çœ‹ç¤ºä¾‹ï¼š

```bash
# æŸ¥çœ‹ç”¨æˆ·åå¥½ç¤ºä¾‹
head -20 data/ml-100k/llm_generated/user_preferences.json

# æŸ¥çœ‹ç‰©å“æè¿°ç¤ºä¾‹
head -20 data/ml-100k/llm_generated/item_descriptions.json
```

---

## æ¨¡å‹è®­ç»ƒ

### 1. åŸºç¡€è®­ç»ƒï¼ˆæ–‡æœ¬æ¨¡æ€ï¼‰

```bash
python scripts/train_v2.py \
    --config configs/movielens_100k.yaml \
    --data_dir data/ml-100k \
    --llm_data_dir data/ml-100k/llm_generated \
    --output_dir outputs/ml-100k \
    --epochs_per_stage 10
```

### 2. å¤šæ¨¡æ€è®­ç»ƒï¼ˆæ–‡æœ¬+å›¾åƒï¼‰

```bash
python scripts/train_v2.py \
    --config configs/movielens_100k.yaml \
    --data_dir data/ml-100k \
    --llm_data_dir data/ml-100k/llm_generated \
    --output_dir outputs/ml-100k-multimodal \
    --use_multimodal \
    --epochs_per_stage 10
```

### 3. è‡ªå®šä¹‰è®­ç»ƒé˜¶æ®µ

åªè®­ç»ƒæŸäº›é˜¶æ®µï¼š

```bash
# åªé¢„è®­ç»ƒ SASRec
python scripts/train_v2.py \
    --config configs/movielens_100k.yaml \
    --data_dir data/ml-100k \
    --llm_data_dir data/ml-100k/llm_generated \
    --output_dir outputs/ml-100k-sasrec \
    --stages pretrain_sasrec

# é¢„è®­ç»ƒåè”åˆå¾®è°ƒ
python scripts/train_v2.py \
    --config configs/movielens_100k.yaml \
    --data_dir data/ml-100k \
    --llm_data_dir data/ml-100k/llm_generated \
    --output_dir outputs/ml-100k-joint \
    --stages pretrain_sasrec pretrain_retriever joint_finetune

# å®Œæ•´å››é˜¶æ®µè®­ç»ƒ
python scripts/train_v2.py \
    --config configs/movielens_100k.yaml \
    --data_dir data/ml-100k \
    --llm_data_dir data/ml-100k/llm_generated \
    --output_dir outputs/ml-100k-full \
    --stages pretrain_sasrec pretrain_retriever joint_finetune end_to_end
```

### 4. è®­ç»ƒå‚æ•°è°ƒæ•´

```bash
python scripts/train_v2.py \
    --config configs/movielens_100k.yaml \
    --data_dir data/ml-100k \
    --llm_data_dir data/ml-100k/llm_generated \
    --output_dir outputs/ml-100k \
    --epochs_per_stage 20 \      # æ¯é˜¶æ®µè®­ç»ƒè½®æ•°
    --patience 10 \               # æ—©åœè€å¿ƒå€¼
    --device cuda \               # ä½¿ç”¨ GPU
    --seed 42                     # éšæœºç§å­
```

---

## è®­ç»ƒé˜¶æ®µè¯¦è§£

### é˜¶æ®µ 1: é¢„è®­ç»ƒ SASRec

**ç›®æ ‡**ï¼šè®­ç»ƒåºåˆ—æ¨èçš„åŸºç¡€èƒ½åŠ›

**è®­ç»ƒå†…å®¹**ï¼š
- åªè®­ç»ƒ SASRec å‚æ•°
- æ£€ç´¢å™¨å‚æ•°å†»ç»“
- ä½¿ç”¨ BPR æŸå¤±

**æŸå¤±å‡½æ•°**ï¼š
```python
loss = -log(sigmoid(pos_score - neg_score))
```

**ä½•æ—¶ä½¿ç”¨**ï¼š
- é¦–æ¬¡è®­ç»ƒæ¨¡å‹
- éœ€è¦å¼ºåŒ–åºåˆ—å»ºæ¨¡èƒ½åŠ›

**æ£€æŸ¥ç‚¹**ï¼š`pretrain_sasrec_best.pt`

---

### é˜¶æ®µ 2: é¢„è®­ç»ƒæ£€ç´¢å™¨

**ç›®æ ‡**ï¼šè®­ç»ƒæ–‡æœ¬åå¥½åŒ¹é…èƒ½åŠ›

**è®­ç»ƒå†…å®¹**ï¼š
- SASRec å‚æ•°å†»ç»“
- åªè®­ç»ƒæ£€ç´¢å™¨å‚æ•°
- ä½¿ç”¨æ£€ç´¢æŸå¤±ï¼ˆBCEï¼‰

**æŸå¤±å‡½æ•°**ï¼š
```python
loss = BCE(retriever_scores, labels)
```

**ä½•æ—¶ä½¿ç”¨**ï¼š
- SASRec å·²é¢„è®­ç»ƒå¥½
- éœ€è¦ä¼˜åŒ–æ–‡æœ¬æ£€ç´¢æ€§èƒ½

**æ£€æŸ¥ç‚¹**ï¼š`pretrain_retriever_best.pt`

---

### é˜¶æ®µ 3: è”åˆå¾®è°ƒ

**ç›®æ ‡**ï¼šåè°ƒä¸¤ä¸ªæ¨¡å—çš„è¾“å‡º

**è®­ç»ƒå†…å®¹**ï¼š
- äº¤æ›¿è®­ç»ƒ SASRec å’Œæ£€ç´¢å™¨
- å¥‡æ•° batch è®­ç»ƒ SASRec
- å¶æ•° batch è®­ç»ƒæ£€ç´¢å™¨

**æŸå¤±å‡½æ•°**ï¼š
```python
loss = sasrec_weight * sasrec_loss + retriever_weight * retriever_loss
```

**ä½•æ—¶ä½¿ç”¨**ï¼š
- ä¸¤ä¸ªæ¨¡å—éƒ½å·²é¢„è®­ç»ƒ
- éœ€è¦å¹³è¡¡ä¸¤è·¯è¾“å‡º

**æ£€æŸ¥ç‚¹**ï¼š`joint_finetune_best.pt`

---

### é˜¶æ®µ 4: ç«¯åˆ°ç«¯ä¼˜åŒ–

**ç›®æ ‡**ï¼šå…¨å±€æœ€ä¼˜åŒ–

**è®­ç»ƒå†…å®¹**ï¼š
- æ‰€æœ‰å‚æ•°ä¸€èµ·è®­ç»ƒ
- ä½¿ç”¨å®Œæ•´æŸå¤±å‡½æ•°
- æ”¯æŒä¸ç¡®å®šæ€§åŠ æƒ

**æŸå¤±å‡½æ•°**ï¼š

å¦‚æœä½¿ç”¨ä¸ç¡®å®šæ€§åŠ æƒï¼š
```python
loss = Î£ (1/(2Ïƒ_iÂ²)) * L_i + log(Ïƒ_iÂ²)
```

å¦åˆ™ï¼š
```python
loss = Î± * sasrec_loss + Î² * retriever_loss
```

å¦‚æœå¤šæ¨¡æ€ï¼š
```python
loss = retrieval_loss +
       consistency_weight * consistency_loss +
       contrastive_weight * contrastive_loss +
       diversity_weight * diversity_loss
```

**ä½•æ—¶ä½¿ç”¨**ï¼š
- å‰ä¸‰é˜¶æ®µéƒ½å·²å®Œæˆ
- è¿½æ±‚æœ€ä½³æ€§èƒ½

**æ£€æŸ¥ç‚¹**ï¼š`end_to_end_best.pt`

---

## é…ç½®æ–‡ä»¶è¯´æ˜

### é…ç½®æ–‡ä»¶ç»“æ„

```yaml
# configs/movielens_100k.yaml

dataset:
  name: "MovieLens-100K"
  num_users: 943
  num_items: 1682

model:
  # SASRec å‚æ•°
  sasrec_hidden_dim: 256
  sasrec_num_blocks: 2
  sasrec_num_heads: 4
  sasrec_dropout: 0.1

  # æ–‡æœ¬ç¼–ç å™¨å‚æ•°
  text_model_name: "all-MiniLM-L6-v2"
  text_embedding_dim: 384
  retriever_output_dim: 256

  # èåˆå‚æ•°
  fusion_method: "weighted"  # weighted | rank | cascade
  sasrec_weight: 0.5
  retriever_weight: 0.5

  # åºåˆ—å‚æ•°
  max_seq_len: 50

training:
  # ä¼˜åŒ–å™¨
  sasrec_lr: 0.001
  retriever_lr: 0.0001
  weight_decay: 0.00001

  # æŸå¤±å‡½æ•°
  use_uncertainty_weighting: true
  retrieval_loss_weight: 1.0
  consistency_weight: 0.1
  contrastive_weight: 0.1
  diversity_weight: 0.01

  # è®­ç»ƒç­–ç•¥
  gradient_clip: 1.0
  warmup_steps: 100

  # æ‰¹æ¬¡å¤§å°
  train_batch_size: 32
  eval_batch_size: 64

  # è´Ÿé‡‡æ ·
  num_negatives: 5
  num_candidates: 100
```

### å…³é”®å‚æ•°è¯´æ˜

#### SASRec å‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|-----|------|--------|
| `sasrec_hidden_dim` | éšè—å±‚ç»´åº¦ | 128-512 |
| `sasrec_num_blocks` | Transformer å±‚æ•° | 2-4 |
| `sasrec_num_heads` | æ³¨æ„åŠ›å¤´æ•° | 2-8 |
| `sasrec_dropout` | Dropout ç‡ | 0.1-0.3 |

#### æ£€ç´¢å™¨å‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|-----|------|--------|
| `text_model_name` | Sentence-BERT æ¨¡å‹ | all-MiniLM-L6-v2 |
| `text_embedding_dim` | æ–‡æœ¬åµŒå…¥ç»´åº¦ | 384 (ç”±æ¨¡å‹å†³å®š) |
| `retriever_output_dim` | æ£€ç´¢å™¨è¾“å‡ºç»´åº¦ | 128-512 |

#### èåˆæ–¹æ³•

- `weighted`: åŠ æƒèåˆï¼ˆæ¨èï¼‰
  ```python
  final_score = Î± * sasrec_score + Î² * retriever_score
  ```

- `rank`: åŸºäºæ’åèåˆ
  ```python
  final_score = Î± * rank_score(sasrec) + Î² * rank_score(retriever)
  ```

- `cascade`: çº§è”èåˆ
  ```python
  final_score = sasrec_score + 0.5 * retriever_score
  ```

#### è®­ç»ƒç­–ç•¥

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|-----|------|--------|
| `sasrec_lr` | SASRec å­¦ä¹ ç‡ | 1e-3 |
| `retriever_lr` | æ£€ç´¢å™¨å­¦ä¹ ç‡ | 1e-4 |
| `use_uncertainty_weighting` | è‡ªåŠ¨ä»»åŠ¡åŠ æƒ | true |
| `gradient_clip` | æ¢¯åº¦è£å‰ª | 1.0 |
| `warmup_steps` | é¢„çƒ­æ­¥æ•° | 100 |

---

## å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒæ—¶å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

**A**: é™ä½æ‰¹æ¬¡å¤§å°æˆ–æ¨¡å‹ç»´åº¦

```yaml
# åœ¨é…ç½®æ–‡ä»¶ä¸­è°ƒæ•´
training:
  train_batch_size: 16  # ä» 32 é™ä½åˆ° 16
  eval_batch_size: 32   # ä» 64 é™ä½åˆ° 32

model:
  sasrec_hidden_dim: 128  # ä» 256 é™ä½åˆ° 128
  retriever_output_dim: 128  # ä» 256 é™ä½åˆ° 128
```

### Q2: LLM API è°ƒç”¨æˆæœ¬å¤ªé«˜æ€ä¹ˆåŠï¼Ÿ

**A**: ä½¿ç”¨ Mock ç”Ÿæˆå™¨æˆ–é™åˆ¶ç”Ÿæˆæ•°é‡

```bash
# ä½¿ç”¨ Mockï¼ˆæ— éœ€ APIï¼‰
--llm_backend mock

# é™åˆ¶ç”Ÿæˆæ•°é‡
--max_users 1000
--max_items 2000
```

### Q3: è®­ç»ƒå¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ

**A**: å‡ ç§åŠ é€Ÿæ–¹æ³•

1. **ä½¿ç”¨ GPU**:
```bash
--device cuda
```

2. **å‡å°‘è®­ç»ƒè½®æ•°**:
```bash
--epochs_per_stage 5
```

3. **è·³è¿‡æŸäº›é˜¶æ®µ**:
```bash
# åªåšç«¯åˆ°ç«¯è®­ç»ƒ
--stages end_to_end
```

4. **å¢åŠ æ‰¹æ¬¡å¤§å°**ï¼ˆå¦‚æœå†…å­˜å…è®¸ï¼‰:
```yaml
training:
  train_batch_size: 64
```

### Q4: å¦‚ä½•æ¢å¤ä¸­æ–­çš„è®­ç»ƒï¼Ÿ

**A**: ä½¿ç”¨æ£€æŸ¥ç‚¹æ¢å¤

```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­åŠ è½½æ£€æŸ¥ç‚¹
trainer.load_checkpoint('outputs/ml-100k/pretrain_sasrec_best.pt')
```

### Q5: å¦‚ä½•è°ƒè¯•æ¨¡å‹æ€§èƒ½ï¼Ÿ

**A**: æŸ¥çœ‹è®­ç»ƒæ—¥å¿—å’ŒæŒ‡æ ‡

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè¾“å‡ºï¼š
```
Epoch 1/10
è®­ç»ƒæŒ‡æ ‡:
  total_loss: 0.5432
  sasrec_loss: 0.3210
  retriever_loss: 0.2222
  lr_sasrec: 0.001
  lr_retriever: 0.0001

éªŒè¯æŒ‡æ ‡:
  hit@5: 0.1234
  hit@10: 0.2345
  hit@20: 0.3456
  ndcg@5: 0.0987
  ndcg@10: 0.1543
  ndcg@20: 0.2109
  mrr: 0.1876
```

### Q6: å¤šæ¨¡æ€è®­ç»ƒå¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A**: ç¡®è®¤ CLIP æ¨¡å‹å·²å®‰è£…

```bash
pip install transformers pillow
```

å¦‚æœä»ç„¶å¤±è´¥ï¼Œå…ˆè®­ç»ƒæ–‡æœ¬æ¨¡æ€ç‰ˆæœ¬ï¼š
```bash
# ä¸ä½¿ç”¨ --use_multimodal æ ‡å¿—
python scripts/train_v2.py \
    --config configs/movielens_100k.yaml \
    --data_dir data/ml-100k \
    --llm_data_dir data/ml-100k/llm_generated \
    --output_dir outputs/ml-100k
```

### Q7: å¦‚ä½•é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼Ÿ

**A**: æ ¹æ®éªŒè¯é›†æŒ‡æ ‡

è®­ç»ƒè„šæœ¬ä¼šè‡ªåŠ¨ä¿å­˜æ¯ä¸ªé˜¶æ®µçš„æœ€ä½³æ¨¡å‹ï¼š
- `pretrain_sasrec_best.pt`
- `pretrain_retriever_best.pt`
- `joint_finetune_best.pt`
- `end_to_end_best.pt`

é€šå¸¸ `end_to_end_best.pt` æ€§èƒ½æœ€å¥½ã€‚

æµ‹è¯•é›†è¯„ä¼°ï¼š
```python
# åŠ è½½æœ€ä½³æ¨¡å‹
model.load_state_dict(torch.load('outputs/ml-100k/end_to_end_best.pt'))

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
test_metrics = trainer.evaluate(test_loader)
```

### Q8: å¦‚ä½•è°ƒæ•´èåˆæƒé‡ï¼Ÿ

**A**: åœ¨é…ç½®æ–‡ä»¶ä¸­ä¿®æ”¹

```yaml
model:
  fusion_method: "weighted"
  sasrec_weight: 0.7      # å¢åŠ  SASRec æƒé‡
  retriever_weight: 0.3   # é™ä½æ£€ç´¢å™¨æƒé‡
```

æˆ–è€…ä½¿ç”¨ä¸ç¡®å®šæ€§åŠ æƒè‡ªåŠ¨å­¦ä¹ ï¼š
```yaml
training:
  use_uncertainty_weighting: true
```

### Q9: è®­ç»ƒå®Œæˆåå¦‚ä½•ä½¿ç”¨æ¨¡å‹ï¼Ÿ

**A**: åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†

```python
import torch
from models.ur4rec_v2 import UR4RecV2

# åŠ è½½æ¨¡å‹
model = UR4RecV2(num_items=1682, ...)
model.load_state_dict(torch.load('outputs/ml-100k/final_model.pt'))
model.eval()

# åŠ è½½ LLM ç”Ÿæˆçš„æ•°æ®
model.load_llm_generated_data(
    'data/ml-100k/llm_generated/user_preferences.json',
    'data/ml-100k/llm_generated/item_descriptions.json'
)

# æ¨ç†
with torch.no_grad():
    ranked_items = model.predict(
        user_ids=[1, 2, 3],
        input_seq=input_seq,
        candidate_items=candidate_items
    )
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æ•°æ®å‡†å¤‡
- âœ… ç¡®ä¿æ•°æ®é¢„å¤„ç†æ­£ç¡®
- âœ… éªŒè¯ LLM ç”Ÿæˆçš„æ–‡æœ¬è´¨é‡
- âœ… æ£€æŸ¥æ•°æ®åˆ†å¸ƒï¼ˆç”¨æˆ·æ´»è·ƒåº¦ã€ç‰©å“æµè¡Œåº¦ï¼‰

### 2. è®­ç»ƒç­–ç•¥
- âœ… å…ˆå¿«é€Ÿè®­ç»ƒä¸€ä¸ªå°è§„æ¨¡ç‰ˆæœ¬ï¼ˆå‡å°‘ç”¨æˆ·/ç‰©å“æ•°ï¼‰
- âœ… éªŒè¯ä»£ç å’Œæµç¨‹æ²¡é—®é¢˜åå†å…¨é‡è®­ç»ƒ
- âœ… ä½¿ç”¨æ—©åœé¿å…è¿‡æ‹Ÿåˆ
- âœ… å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹

### 3. è¶…å‚æ•°è°ƒä¼˜
- âœ… å…ˆç”¨é»˜è®¤å‚æ•°è®­ç»ƒ
- âœ… ç„¶åè°ƒæ•´å­¦ä¹ ç‡
- âœ… æœ€åè°ƒæ•´æ¨¡å‹ç»´åº¦å’Œå±‚æ•°

### 4. æ€§èƒ½ä¼˜åŒ–
- âœ… ä½¿ç”¨ GPU åŠ é€Ÿ
- âœ… åˆç†è®¾ç½®æ‰¹æ¬¡å¤§å°
- âœ… ä½¿ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½

### 5. ç»“æœåˆ†æ
- âœ… å¯¹æ¯”ä¸åŒé˜¶æ®µçš„æ€§èƒ½
- âœ… åˆ†æ SASRec å’Œæ£€ç´¢å™¨çš„è´¡çŒ®
- âœ… å¯è§†åŒ–è®­ç»ƒæ›²çº¿

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [README_CN.md](README_CN.md) - é¡¹ç›®æ€»è§ˆ
- [QUICKSTART_CN.md](QUICKSTART_CN.md) - å¿«é€Ÿå¼€å§‹
- [REFACTORING_PROGRESS.md](REFACTORING_PROGRESS.md) - é‡æ„è¿›åº¦
- [PROJECT_SUMMARY_CN.md](PROJECT_SUMMARY_CN.md) - æŠ€æœ¯ç»†èŠ‚

---

**æœ€åæ›´æ–°**: 2025-11-27
